"""
æ™ºèƒ½å®¢æœç³»ç»Ÿç¤ºä¾‹
å±•ç¤º LangChain å’Œ LangGraph çš„å®Œæ•´é…åˆ
"""
from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage


# ============ LangChain ç»„ä»¶å±‚ ============

# 1. LLM æ¨¡å‹
llm = init_chat_model("deepseek:deepseek-chat", temperature=0.7)

# 2. å·¥å…·å®šä¹‰
@tool
def query_order_status(order_id: str) -> str:
    """æŸ¥è¯¢è®¢å•çŠ¶æ€"""
    # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
    orders = {
        "12345": "å·²å‘è´§ï¼Œé¢„è®¡æ˜å¤©é€è¾¾",
        "67890": "å¤„ç†ä¸­ï¼Œé¢„è®¡ä»Šå¤©å‘è´§"
    }
    return orders.get(order_id, "è®¢å•ä¸å­˜åœ¨")


@tool
def query_product_info(product_name: str) -> str:
    """æŸ¥è¯¢äº§å“ä¿¡æ¯"""
    products = {
        "æ‰‹æœº": "æœ€æ–°æ¬¾æ™ºèƒ½æ‰‹æœºï¼Œä»·æ ¼ 3999 å…ƒ",
        "ç”µè„‘": "é«˜æ€§èƒ½ç¬”è®°æœ¬ç”µè„‘ï¼Œä»·æ ¼ 8999 å…ƒ"
    }
    return products.get(product_name, "äº§å“ä¸å­˜åœ¨")


@tool
def create_ticket(issue: str) -> str:
    """åˆ›å»ºå·¥å•"""
    ticket_id = "TICKET-" + str(hash(issue))[:6]
    return f"å·²åˆ›å»ºå·¥å• {ticket_id}ï¼Œå®¢æœå°†åœ¨ 24 å°æ—¶å†…è”ç³»æ‚¨"


# 3. æç¤ºè¯æ¨¡æ¿
classifier_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªå®¢æœåˆ†ç±»å™¨ã€‚åˆ†æç”¨æˆ·é—®é¢˜ï¼Œè¿”å›ç±»åˆ«ï¼š
    - order: è®¢å•ç›¸å…³é—®é¢˜
    - product: äº§å“å’¨è¯¢
    - complaint: æŠ•è¯‰å»ºè®®
    - general: ä¸€èˆ¬å’¨è¯¢
    
    åªè¿”å›ä¸€ä¸ªè¯ï¼Œä¸è¦è§£é‡Šã€‚"""),
    ("human", "{question}")
])

order_agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯è®¢å•å¤„ç†ä¸“å®¶ã€‚ä½¿ç”¨ query_order_status å·¥å…·æŸ¥è¯¢è®¢å•ã€‚
    å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾›è®¢å•å·ï¼Œè¯·è¯¢é—®è®¢å•å·ã€‚"""),
    ("human", "{question}")
])

product_agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯äº§å“é¡¾é—®ã€‚ä½¿ç”¨ query_product_info å·¥å…·æŸ¥è¯¢äº§å“ä¿¡æ¯ã€‚
    æä¾›ä¸“ä¸šçš„äº§å“å»ºè®®ã€‚"""),
    ("human", "{question}")
])

complaint_agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯æŠ•è¯‰å¤„ç†ä¸“å‘˜ã€‚è®¤çœŸå€¾å¬ç”¨æˆ·é—®é¢˜ï¼Œä½¿ç”¨ create_ticket åˆ›å»ºå·¥å•ã€‚
    è¡¨è¾¾æ­‰æ„å¹¶æ‰¿è¯ºè·Ÿè¿›ã€‚"""),
    ("human", "{question}")
])


# ============ LangGraph å·¥ä½œæµå±‚ ============

# çŠ¶æ€å®šä¹‰
class CustomerServiceState(TypedDict):
    messages: Annotated[list, add_messages]
    question: str
    category: str
    answer: str
    needs_human: bool


# åˆ›å»ºå·¥ä½œæµ
workflow = StateGraph(CustomerServiceState)


# èŠ‚ç‚¹ 1: åˆ†ç±»å™¨ï¼ˆä½¿ç”¨ LangChain LLMï¼‰
def classify_question(state: CustomerServiceState):
    """åˆ†ç±»ç”¨æˆ·é—®é¢˜"""
    question = state["question"]
    
    # ä½¿ç”¨ LangChain çš„ LCEL
    chain = classifier_prompt | llm
    response = chain.invoke({"question": question})
    
    category = response.content.strip().lower()
    print(f"ğŸ“‹ é—®é¢˜åˆ†ç±»: {category}")
    
    return {"category": category}


# èŠ‚ç‚¹ 2: è®¢å•å¤„ç† Agentï¼ˆä½¿ç”¨ LangChain Toolsï¼‰
def order_agent(state: CustomerServiceState):
    """å¤„ç†è®¢å•é—®é¢˜"""
    from langgraph.prebuilt import create_react_agent
    
    # åˆ›å»ºä¸“é—¨çš„è®¢å• Agent
    agent = create_react_agent(
        model=llm,
        tools=[query_order_status],
        state_modifier=order_agent_prompt.format(question=state["question"])
    )
    
    result = agent.invoke({"messages": [HumanMessage(content=state["question"])]})
    answer = result["messages"][-1].content
    
    print(f"ğŸ“¦ è®¢å• Agent å›å¤: {answer[:50]}...")
    return {"answer": answer}


# èŠ‚ç‚¹ 3: äº§å“å’¨è¯¢ Agent
def product_agent(state: CustomerServiceState):
    """å¤„ç†äº§å“å’¨è¯¢"""
    from langgraph.prebuilt import create_react_agent
    
    agent = create_react_agent(
        model=llm,
        tools=[query_product_info],
        state_modifier=product_agent_prompt.format(question=state["question"])
    )
    
    result = agent.invoke({"messages": [HumanMessage(content=state["question"])]})
    answer = result["messages"][-1].content
    
    print(f"ğŸ›ï¸ äº§å“ Agent å›å¤: {answer[:50]}...")
    return {"answer": answer}


# èŠ‚ç‚¹ 4: æŠ•è¯‰å¤„ç† Agent
def complaint_agent(state: CustomerServiceState):
    """å¤„ç†æŠ•è¯‰"""
    from langgraph.prebuilt import create_react_agent
    
    agent = create_react_agent(
        model=llm,
        tools=[create_ticket],
        state_modifier=complaint_agent_prompt.format(question=state["question"])
    )
    
    result = agent.invoke({"messages": [HumanMessage(content=state["question"])]})
    answer = result["messages"][-1].content
    
    print(f"ğŸ“ æŠ•è¯‰ Agent å›å¤: {answer[:50]}...")
    return {"answer": answer}


# èŠ‚ç‚¹ 5: é€šç”¨å®¢æœ
def general_agent(state: CustomerServiceState):
    """å¤„ç†ä¸€èˆ¬å’¨è¯¢"""
    chain = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯å‹å¥½çš„å®¢æœåŠ©æ‰‹ï¼Œå›ç­”ç”¨æˆ·çš„ä¸€èˆ¬é—®é¢˜"),
        ("human", "{question}")
    ]) | llm
    
    response = chain.invoke({"question": state["question"]})
    answer = response.content
    
    print(f"ğŸ’¬ é€šç”¨ Agent å›å¤: {answer[:50]}...")
    return {"answer": answer}


# è·¯ç”±å‡½æ•°
def route_to_agent(state: CustomerServiceState) -> Literal["order", "product", "complaint", "general"]:
    """æ ¹æ®åˆ†ç±»è·¯ç”±åˆ°ä¸åŒçš„ Agent"""
    category = state.get("category", "general")
    
    if category == "order":
        return "order"
    elif category == "product":
        return "product"
    elif category == "complaint":
        return "complaint"
    else:
        return "general"


# ============ æ„å»ºå·¥ä½œæµ ============

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("classify", classify_question)
workflow.add_node("order", order_agent)
workflow.add_node("product", product_agent)
workflow.add_node("complaint", complaint_agent)
workflow.add_node("general", general_agent)

# å®šä¹‰æµç¨‹
workflow.add_edge(START, "classify")

# æ¡ä»¶è·¯ç”±
workflow.add_conditional_edges(
    "classify",
    route_to_agent,
    {
        "order": "order",
        "product": "product",
        "complaint": "complaint",
        "general": "general"
    }
)

# æ‰€æœ‰ Agent éƒ½åˆ°ç»“æŸ
workflow.add_edge("order", END)
workflow.add_edge("product", END)
workflow.add_edge("complaint", END)
workflow.add_edge("general", END)

# ç¼–è¯‘ï¼ˆæ·»åŠ æŒä¹…åŒ–ï¼‰
checkpointer = SqliteSaver.from_conn_string(":memory:")
customer_service_app = workflow.compile(checkpointer=checkpointer)


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def chat(question: str, thread_id: str = "user-001"):
    """ä¸å®¢æœç³»ç»Ÿå¯¹è¯"""
    print(f"\n{'='*60}")
    print(f"ğŸ‘¤ ç”¨æˆ·: {question}")
    print(f"{'='*60}")
    
    config = {"configurable": {"thread_id": thread_id}}
    
    result = customer_service_app.invoke(
        {
            "question": question,
            "messages": [],
            "category": "",
            "answer": "",
            "needs_human": False
        },
        config=config
    )
    
    print(f"\nğŸ¤– å®¢æœ: {result['answer']}")
    print(f"{'='*60}\n")
    
    return result


if __name__ == "__main__":
    # æµ‹è¯•ä¸åŒç±»å‹çš„é—®é¢˜
    
    # 1. è®¢å•æŸ¥è¯¢
    chat("æˆ‘çš„è®¢å• 12345 åˆ°å“ªäº†ï¼Ÿ")
    
    # 2. äº§å“å’¨è¯¢
    chat("ä½ ä»¬çš„æ‰‹æœºæ€ä¹ˆæ ·ï¼Ÿ")
    
    # 3. æŠ•è¯‰
    chat("æˆ‘æ”¶åˆ°çš„å•†å“æœ‰è´¨é‡é—®é¢˜ï¼Œè¦æ±‚é€€æ¬¾ï¼")
    
    # 4. ä¸€èˆ¬å’¨è¯¢
    chat("ä½ ä»¬çš„è¥ä¸šæ—¶é—´æ˜¯ï¼Ÿ")
    
    # å¯è§†åŒ–å·¥ä½œæµ
    try:
        from IPython.display import Image, display
        display(Image(customer_service_app.get_graph().draw_mermaid_png()))
    except:
        print("æç¤ºï¼šåœ¨ Jupyter ç¯å¢ƒä¸­å¯ä»¥å¯è§†åŒ–å·¥ä½œæµ")

