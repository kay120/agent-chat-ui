#!/usr/bin/env python3
"""
çœŸæ­£çš„ LangGraph æœåŠ¡å™¨ - ä½¿ç”¨å®Œæ•´çš„ LangGraph æ¡†æ¶
"""

import os
import json
import uuid
import asyncio
from typing import Dict, Any, List, Optional, Annotated
from datetime import datetime
import weakref

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# LangChain å’Œ LangGraph å¯¼å…¥
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from typing_extensions import TypedDict

# DeepSeek æ¨¡å‹å®ç°
import httpx
import time

# é…ç½®
from dotenv import load_dotenv
load_dotenv()

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

print(f"ğŸ”‘ Loaded API Key: {DEEPSEEK_API_KEY[:10] if DEEPSEEK_API_KEY else 'None'}...")
print(f"ğŸŒ Base URL: {DEEPSEEK_BASE_URL}")

# å®šä¹‰çŠ¶æ€
class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

# è‡ªå®šä¹‰ DeepSeek èŠå¤©æ¨¡å‹
class DeepSeekChatModel:
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com"):
        self.api_key = api_key
        self.base_url = base_url
    
    async def ainvoke(self, messages: List[BaseMessage]) -> AIMessage:
        """å¼‚æ­¥è°ƒç”¨ DeepSeek API"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            api_messages = []
            for msg in messages:
                if isinstance(msg, HumanMessage):
                    api_messages.append({"role": "user", "content": str(msg.content)})
                elif isinstance(msg, AIMessage):
                    api_messages.append({"role": "assistant", "content": str(msg.content)})

            print(f"ğŸ”‘ API Key: {self.api_key[:10]}...")
            print(f"ğŸ“ API Messages: {api_messages}")

            timeout = httpx.Timeout(120.0, connect=30.0)  # å¢åŠ è¿æ¥å’Œæ€»è¶…æ—¶æ—¶é—´
            async with httpx.AsyncClient(timeout=timeout) as client:
                print(f"ğŸŒ Making request to: {self.base_url}/chat/completions")
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "deepseek-chat",
                        "messages": api_messages,
                        "stream": True
                    }
                )

                print(f"ğŸŒ Response status: {response.status_code}")
                if response.status_code != 200:
                    print(f"âŒ Response text: {response.text}")
                    return AIMessage(content="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•")

                # å¤„ç†æµå¼å“åº”
                content_parts = []
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]  # å»æ‰ "data: " å‰ç¼€
                        if data_str.strip() == "[DONE]":
                            break
                        try:
                            data = json.loads(data_str)
                            if "choices" in data and len(data["choices"]) > 0:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    content_parts.append(delta["content"])
                                    print(f"ğŸ“ Stream chunk: {delta['content']}")
                        except json.JSONDecodeError:
                            continue

                full_content = "".join(content_parts)
                print(f"âœ… Complete AI Response: {full_content[:100]}...")
                return AIMessage(content=full_content)

        except Exception as e:
            print(f"âŒ DeepSeek API Error: {e}")
            print(f"âŒ Error type: {type(e)}")
            import traceback
            print(f"âŒ Full traceback: {traceback.format_exc()}")
            return AIMessage(content=f"ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçœŸæ­£LangGraphæ¡†æ¶çš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼")

    async def astream(self, messages: List[BaseMessage], stream_callback=None) -> AIMessage:
        """å¼‚æ­¥æµå¼è°ƒç”¨ DeepSeek API"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            api_messages = []
            for msg in messages:
                if isinstance(msg, HumanMessage):
                    api_messages.append({"role": "user", "content": str(msg.content)})
                elif isinstance(msg, AIMessage):
                    api_messages.append({"role": "assistant", "content": str(msg.content)})

            print(f"ğŸ”‘ API Key: {self.api_key[:10]}...")
            print(f"ğŸ“ API Messages: {api_messages}")

            timeout = httpx.Timeout(120.0, connect=30.0)  # å¢åŠ è¿æ¥å’Œæ€»è¶…æ—¶æ—¶é—´
            async with httpx.AsyncClient(timeout=timeout) as client:
                print(f"ğŸŒ Making streaming request to: {self.base_url}/chat/completions")
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "deepseek-chat",
                        "messages": api_messages,
                        "stream": True
                    }
                )

                print(f"ğŸŒ Response status: {response.status_code}")
                if response.status_code != 200:
                    print(f"âŒ Response text: {response.text}")
                    return AIMessage(content="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•")

                # å¤„ç†æµå¼å“åº”
                content_parts = []
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]  # å»æ‰ "data: " å‰ç¼€
                        if data_str.strip() == "[DONE]":
                            break
                        try:
                            data = json.loads(data_str)
                            if "choices" in data and len(data["choices"]) > 0:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    chunk_content = delta["content"]
                                    content_parts.append(chunk_content)
                                    print(f"ğŸ“ Stream chunk: {chunk_content}")

                                    # ç«‹å³å›è°ƒæµå¼å†…å®¹
                                    if stream_callback:
                                        await stream_callback(chunk_content)
                        except json.JSONDecodeError:
                            continue

                full_content = "".join(content_parts)
                print(f"âœ… Complete streaming response: {full_content[:100]}...")
                return AIMessage(content=full_content)

        except Exception as e:
            print(f"âŒ DeepSeek Streaming API Error: {e}")
            print(f"âŒ Error type: {type(e)}")
            import traceback
            print(f"âŒ Full traceback: {traceback.format_exc()}")
            return AIMessage(content=f"ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçœŸæ­£LangGraphæ¡†æ¶çš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼")

        except Exception as e:
            print(f"âŒ DeepSeek API Error: {e}")
            print(f"âŒ Error type: {type(e)}")
            import traceback
            print(f"âŒ Full traceback: {traceback.format_exc()}")
            return AIMessage(content=f"ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçœŸæ­£LangGraphæ¡†æ¶çš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼")

# åˆå§‹åŒ–æ¨¡å‹
llm = DeepSeekChatModel(DEEPSEEK_API_KEY)

# å®šä¹‰èŠå¤©æœºå™¨äººèŠ‚ç‚¹
async def chatbot(state: State) -> Dict[str, Any]:
    """èŠå¤©æœºå™¨äººèŠ‚ç‚¹ - è¿™æ˜¯çœŸæ­£çš„ LangGraph èŠ‚ç‚¹"""
    print(f"ğŸ¤– Chatbot node called with {len(state['messages'])} messages")

    # è°ƒç”¨ LLM
    response = await llm.ainvoke(state["messages"])
    print(f"ğŸ¤– LLM response: {response.content[:100]}...")

    # è¿”å›æ–°æ¶ˆæ¯
    return {"messages": [response]}

# æµå¼èŠå¤©æœºå™¨äººèŠ‚ç‚¹
async def streaming_chatbot(state: State, stream_callback=None) -> Dict[str, Any]:
    """æµå¼èŠå¤©æœºå™¨äººèŠ‚ç‚¹"""
    print(f"ğŸ¤– Streaming Chatbot node called with {len(state['messages'])} messages")

    # è°ƒç”¨æµå¼ LLM
    response = await llm.astream(state["messages"], stream_callback)
    print(f"ğŸ¤– Final LLM response: {response.content[:100]}...")

    # è¿”å›æ–°æ¶ˆæ¯
    return {"messages": [response]}

# æ„å»º LangGraph
workflow = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("chatbot", chatbot)

# æ·»åŠ è¾¹
workflow.add_edge(START, "chatbot")
workflow.add_edge("chatbot", END)

# ç¼–è¯‘å›¾
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)

# å…¨å±€å–æ¶ˆç®¡ç†
active_streams: Dict[str, asyncio.Event] = {}

# ç®€å•çš„å†…å­˜å­˜å‚¨å†å²è®°å½•
thread_history: Dict[str, List[Dict]] = {}

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="Real LangGraph Server")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/info")
async def get_info():
    """è·å–æœåŠ¡å™¨ä¿¡æ¯"""
    return {"status": "ok", "framework": "LangGraph", "model": "DeepSeek"}

# æ›´å…·ä½“çš„è·¯ç”±æ”¾åœ¨å‰é¢ï¼Œé¿å…è·¯ç”±å†²çª
@app.post("/threads/{thread_id}/runs/stream")
async def stream_thread_run(thread_id: str, request: Request):
    """çº¿ç¨‹æµå¼è¿è¡Œ"""
    return await stream_run(request, thread_id)

@app.post("/threads/{thread_id}/history")
async def get_history(thread_id: str):
    """è·å–å†å²è®°å½•"""
    return thread_history.get(thread_id, [])

@app.post("/runs/{run_id}/cancel")
async def cancel_run(run_id: str):
    """å–æ¶ˆè¿è¡Œä¸­çš„æµå¼è¯·æ±‚"""
    if run_id in active_streams:
        active_streams[run_id].set()  # è®¾ç½®å–æ¶ˆäº‹ä»¶
        print(f"ğŸ›‘ å–æ¶ˆè¯·æ±‚: {run_id}")
        return {"status": "cancelled", "run_id": run_id}
    else:
        print(f"âŒ æœªæ‰¾åˆ°è¿è¡Œä¸­çš„è¯·æ±‚: {run_id}")
        return {"status": "not_found", "run_id": run_id}

@app.post("/threads")
async def create_thread():
    """åˆ›å»ºæ–°çº¿ç¨‹"""
    return {"thread_id": str(uuid.uuid4())}

@app.get("/threads")
async def list_threads():
    """è·å–æ‰€æœ‰çº¿ç¨‹åˆ—è¡¨"""
    threads = []
    for thread_id, messages in thread_history.items():
        if messages:
            # è·å–æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºé¢„è§ˆ
            last_message = messages[-1]
            preview = ""
            if last_message.get("type") == "human":
                preview = last_message.get("content", "")[:50]
            elif last_message.get("type") == "ai":
                preview = last_message.get("content", "")[:50]

            threads.append({
                "thread_id": thread_id,
                "created_at": "2024-01-01T00:00:00Z",  # ç®€åŒ–æ—¶é—´æˆ³
                "updated_at": "2024-01-01T00:00:00Z",
                "metadata": {},
                "status": "idle",
                "config": {},
                "values": {"messages": messages},
                "preview": preview
            })

    return threads

@app.delete("/threads/{thread_id}")
async def delete_thread(thread_id: str):
    """åˆ é™¤æŒ‡å®šçº¿ç¨‹"""
    if thread_id in thread_history:
        del thread_history[thread_id]
        print(f"ğŸ—‘ï¸ åˆ é™¤çº¿ç¨‹: {thread_id}")
        return {"status": "deleted", "thread_id": thread_id}
    else:
        print(f"âŒ çº¿ç¨‹ä¸å­˜åœ¨: {thread_id}")
        return {"status": "not_found", "thread_id": thread_id}

@app.post("/runs/stream")
async def stream_run(request: Request, thread_id: Optional[str] = None):
    """çœŸæ­£çš„ LangGraph token-by-token æµå¼è¾“å‡º"""
    data = await request.json()
    messages = data.get("input", {}).get("messages", [])

    # å¦‚æœæ²¡æœ‰æä¾› thread_idï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„
    if not thread_id:
        thread_id = str(uuid.uuid4())

    # ç”Ÿæˆè¿è¡Œ ID
    run_id = str(uuid.uuid4())
    cancel_event = asyncio.Event()
    active_streams[run_id] = cancel_event

    print(f"ğŸš€ çœŸæ­£çš„ LangGraph token-by-token æµå¼å¤„ç† (Run ID: {run_id}, Thread ID: {thread_id})")

    async def generate():
        aiContent = ""  # ç”¨äºæ”¶é›†AIå›å¤å†…å®¹
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º LangChain æ ¼å¼
            langchain_messages = []
            for msg in messages:
                content = msg.get("content", "")
                if isinstance(content, list):
                    content = content[0].get("text", "")

                if msg.get("type") == "human":
                    langchain_messages.append(HumanMessage(content=content))

            print(f"ğŸ“ å¤„ç† {len(langchain_messages)} æ¡æ¶ˆæ¯")

            # ç›´æ¥ä½¿ç”¨ DeepSeek API è¿›è¡ŒçœŸæ­£çš„ token-by-token æµå¼å¤„ç†
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            api_messages = []
            for msg in langchain_messages:
                if isinstance(msg, HumanMessage):
                    api_messages.append({"role": "user", "content": str(msg.content)})
                elif isinstance(msg, AIMessage):
                    api_messages.append({"role": "assistant", "content": str(msg.content)})

            print(f"ğŸ“ API Messages: {api_messages}")

            # ä½¿ç”¨ OpenAI SDK è°ƒç”¨ DeepSeek API
            from openai import AsyncOpenAI

            print("ï¿½ ä½¿ç”¨ OpenAI SDK è°ƒç”¨ DeepSeek API...")

            # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼ŒæŒ‡å‘ DeepSeek API
            client = AsyncOpenAI(
                api_key=DEEPSEEK_API_KEY,
                base_url="https://api.deepseek.com"
            )

            print("ğŸ“¡ å¼€å§‹æµå¼è¯·æ±‚...")
            stream = await client.chat.completions.create(
                model="deepseek-chat",
                messages=api_messages,
                stream=True
            )

            print("ğŸ“¡ å¼€å§‹è¯»å–æµå¼å“åº”...")
            async for chunk in stream:
                # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
                if cancel_event.is_set():
                    print(f"ğŸ›‘ æµå¼å¤„ç†è¢«å–æ¶ˆ (Run ID: {run_id})")
                    yield f"data: {json.dumps({'error': 'cancelled'})}\n\n"
                    break

                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    aiContent += content  # ç´¯ç§¯AIå›å¤å†…å®¹
                    print(f"ğŸ“¤ å®æ—¶è½¬å‘ token: '{content}' (æ—¶é—´æˆ³: {time.time()})", flush=True)

                    # ç«‹å³è½¬å‘æ¯ä¸ª token
                    chunk_response = {"content": content}
                    yield f"data: {json.dumps(chunk_response)}\n\n"

            print("âœ… SDK æµå¼å®Œæˆ")

            # ä¿å­˜å†å²è®°å½•
            if thread_id not in thread_history:
                thread_history[thread_id] = []

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            for msg in api_messages:
                if msg["role"] == "user":
                    thread_history[thread_id].append({
                        "id": str(uuid.uuid4()),
                        "type": "human",
                        "content": msg["content"]
                    })

            # æ·»åŠ AIå›å¤
            if aiContent:
                thread_history[thread_id].append({
                    "id": str(uuid.uuid4()),
                    "type": "ai",
                    "content": aiContent
                })

            print(f"ğŸ’¾ ä¿å­˜å†å²è®°å½•åˆ°çº¿ç¨‹: {thread_id}")

        except Exception as e:
            print(f"âŒ LangGraph æµå¼é”™è¯¯: {e}")
            import traceback
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # æ¸…ç†æ´»è·ƒæµ
            if run_id in active_streams:
                del active_streams[run_id]
                print(f"ğŸ§¹ æ¸…ç†æµå¼è¯·æ±‚: {run_id}")

    return StreamingResponse(
        generate(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "X-Accel-Buffering": "no",
            "X-Run-ID": run_id,  # æ·»åŠ  run_id åˆ°å“åº”å¤´
            "X-Thread-ID": thread_id,  # æ·»åŠ  thread_id åˆ°å“åº”å¤´
        }
    )

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨çœŸæ­£çš„ LangGraph æœåŠ¡å™¨...")
    print("ğŸ“ æœåŠ¡å™¨åœ°å€: http://localhost:2024")
    print("ğŸ¤– ä½¿ç”¨ LangGraph æ¡†æ¶ + DeepSeek æ¨¡å‹")
    uvicorn.run(app, host="0.0.0.0", port=2024)
