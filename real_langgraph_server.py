#!/usr/bin/env python3
"""
çœŸæ­£çš„ LangGraph æœåŠ¡å™¨ - åŸºäºæ•™ç¨‹çš„æ­£ç¡®å®ç°
"""

import os
import json
import uuid
import asyncio
from typing import Dict, Any, List, Optional, Annotated
from datetime import datetime

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# LangChain å’Œ LangGraph å¯¼å…¥
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from typing_extensions import TypedDict
from pydantic import BaseModel

# ç¯å¢ƒå˜é‡
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not DEEPSEEK_API_KEY:
    raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")

# FastAPI åº”ç”¨
app = FastAPI(title="LangGraph Chat Server", version="1.0.0")

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å®šä¹‰çŠ¶æ€ - åŸºäºæ•™ç¨‹çš„æ­£ç¡®æ–¹å¼
class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

# åˆ›å»º DeepSeek æ¨¡å‹å®ä¾‹ï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰
llm = ChatOpenAI(
    model="deepseek-chat",
    api_key=DEEPSEEK_API_KEY,
    base_url="https://api.deepseek.com",
    temperature=0.7,
    streaming=True  # å¯ç”¨æµå¼å¤„ç†
)

# å®šä¹‰èŠå¤©æœºå™¨äººèŠ‚ç‚¹ - åŸºäºæ•™ç¨‹çš„æ­£ç¡®æ–¹å¼
async def chatbot_node(state: State) -> Dict[str, Any]:
    """èŠå¤©æœºå™¨äººèŠ‚ç‚¹ - çœŸæ­£çš„ LangGraph èŠ‚ç‚¹"""
    print(f"ğŸ¤– Chatbot node called with {len(state['messages'])} messages")
    
    # è°ƒç”¨ LLM - è¿™é‡Œä½¿ç”¨ LangChain çš„ ChatOpenAI
    response = await llm.ainvoke(state["messages"])
    print(f"ğŸ¤– LLM response: {response.content[:100]}...")
    
    # è¿”å›æ–°æ¶ˆæ¯ - LangGraph ä¼šè‡ªåŠ¨åˆå¹¶åˆ°çŠ¶æ€ä¸­
    return {"messages": [response]}

# æ„å»º LangGraph - åŸºäºæ•™ç¨‹çš„æ­£ç¡®æ–¹å¼
workflow = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("chatbot", chatbot_node)

# æ·»åŠ è¾¹
workflow.add_edge(START, "chatbot")
workflow.add_edge("chatbot", END)

# æ·»åŠ å†…å­˜æ£€æŸ¥ç‚¹
memory = MemorySaver()

# ç¼–è¯‘å›¾
graph = workflow.compile(checkpointer=memory)

# çº¿ç¨‹å†å²å­˜å‚¨
thread_history: Dict[str, List[Dict[str, Any]]] = {}

# API æ¨¡å‹
class ChatMessage(BaseModel):
    content: str
    role: str = "user"

class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None

class ThreadInfo(BaseModel):
    thread_id: str
    created_at: str
    message_count: int
    last_message: str

@app.get("/")
async def root():
    return {"message": "LangGraph Chat Server is running!"}

@app.get("/info")
async def info():
    return {
        "server": "LangGraph Chat Server",
        "version": "1.0.0",
        "framework": "LangGraph + FastAPI",
        "model": "DeepSeek Chat",
        "features": [
            "çœŸæ­£çš„ LangGraph å®ç°",
            "æµå¼è¾“å‡ºæ”¯æŒ",
            "çº¿ç¨‹ç®¡ç†",
            "çŠ¶æ€æŒä¹…åŒ–",
            "å†…å­˜æ£€æŸ¥ç‚¹"
        ]
    }

@app.get("/threads")
async def get_threads():
    """è·å–æ‰€æœ‰çº¿ç¨‹ - è¿”å›ç¬¦åˆ LangGraph SDK æ ¼å¼çš„æ•°æ®"""
    threads = []
    for thread_id, messages in thread_history.items():
        if messages:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º LangGraph SDK æœŸæœ›çš„æ ¼å¼
            langgraph_messages = []
            for msg in messages:
                if msg["role"] == "user":
                    langgraph_messages.append({
                        "type": "human",
                        "content": msg["content"]
                    })
                elif msg["role"] == "assistant":
                    langgraph_messages.append({
                        "type": "ai",
                        "content": msg["content"]
                    })

            first_msg = messages[0]
            last_msg = messages[-1]

            # æ„å»ºç¬¦åˆ LangGraph SDK Thread æ ¼å¼çš„å¯¹è±¡
            thread_obj = {
                "thread_id": thread_id,
                "created_at": first_msg.get("timestamp", ""),
                "updated_at": last_msg.get("timestamp", ""),
                "metadata": {},
                "status": "idle",
                "values": {
                    "messages": langgraph_messages
                },
                "interrupts": {}
            }
            threads.append(thread_obj)

    return {"threads": sorted(threads, key=lambda x: x["created_at"], reverse=True)}

@app.get("/threads/{thread_id}")
async def get_thread_messages(thread_id: str):
    """è·å–ç‰¹å®šçº¿ç¨‹çš„æ¶ˆæ¯"""
    if thread_id not in thread_history:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    return {"messages": thread_history[thread_id]}

@app.delete("/threads/{thread_id}")
async def delete_thread(thread_id: str):
    """åˆ é™¤çº¿ç¨‹"""
    if thread_id not in thread_history:
        raise HTTPException(status_code=404, detail="Thread not found")

    del thread_history[thread_id]
    print(f"ğŸ—‘ï¸ åˆ é™¤çº¿ç¨‹: {thread_id}")
    return {"status": "deleted", "thread_id": thread_id}

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """éæµå¼èŠå¤©ç«¯ç‚¹"""
    try:
        # ç”Ÿæˆæˆ–ä½¿ç”¨ç°æœ‰çº¿ç¨‹ID
        thread_id = request.thread_id or str(uuid.uuid4())
        
        # åˆå§‹åŒ–çº¿ç¨‹å†å²
        if thread_id not in thread_history:
            thread_history[thread_id] = []
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        user_message = {
            "role": "user",
            "content": request.message,
            "timestamp": datetime.now().isoformat()
        }
        thread_history[thread_id].append(user_message)
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        for msg in thread_history[thread_id]:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
        
        # ä½¿ç”¨ LangGraph å¤„ç† - è¿™æ˜¯å…³é”®ï¼
        config = {"configurable": {"thread_id": thread_id}}
        result = await graph.ainvoke({"messages": messages}, config)
        
        # è·å–AIå“åº”
        ai_response = result["messages"][-1].content
        
        # æ·»åŠ AIå“åº”åˆ°å†å²
        ai_message = {
            "role": "assistant", 
            "content": ai_response,
            "timestamp": datetime.now().isoformat()
        }
        thread_history[thread_id].append(ai_message)
        
        return {
            "response": ai_response,
            "thread_id": thread_id
        }
        
    except Exception as e:
        print(f"âŒ Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/stream")
async def stream_chat_endpoint(request: ChatRequest):
    """æµå¼èŠå¤©ç«¯ç‚¹ - ä½¿ç”¨çœŸæ­£çš„ LangGraph æµå¼å¤„ç†"""
    try:
        # ç”Ÿæˆæˆ–ä½¿ç”¨ç°æœ‰çº¿ç¨‹ID
        thread_id = request.thread_id or str(uuid.uuid4())

        # åˆå§‹åŒ–çº¿ç¨‹å†å²
        if thread_id not in thread_history:
            thread_history[thread_id] = []

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        user_message = {
            "role": "user",
            "content": request.message,
            "timestamp": datetime.now().isoformat()
        }
        thread_history[thread_id].append(user_message)

        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        for msg in thread_history[thread_id]:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))

        async def generate_stream():
            try:
                # å‘é€çº¿ç¨‹ID
                yield f"data: {json.dumps({'type': 'thread_id', 'thread_id': thread_id})}\n\n"

                # ä½¿ç”¨ LangGraph çš„æµå¼å¤„ç† - åŸºäºæ•™ç¨‹çš„æ­£ç¡®æ–¹æ³•
                config = {"configurable": {"thread_id": thread_id}}

                # æ”¶é›†å®Œæ•´å“åº”
                full_response = ""

                # ä½¿ç”¨ astream_events è·å–æµå¼äº‹ä»¶ - è¿™æ˜¯å…³é”®ï¼
                async for event in graph.astream_events(
                    {"messages": messages},
                    config,
                    version="v2"
                ):
                    kind = event["event"]

                    # ç›‘å¬èŠå¤©æ¨¡å‹çš„æµå¼è¾“å‡º - åŸºäºæ•™ç¨‹
                    if kind == "on_chat_model_stream":
                        chunk_data = event["data"]
                        if "chunk" in chunk_data and hasattr(chunk_data["chunk"], "content"):
                            chunk_content = chunk_data["chunk"].content
                            if chunk_content:
                                full_response += chunk_content
                                # å‘é€æµå¼æ•°æ®
                                yield f"data: {json.dumps({'type': 'content', 'content': chunk_content})}\n\n"

                # æ·»åŠ å®Œæ•´å“åº”åˆ°å†å²
                if full_response:
                    ai_message = {
                        "role": "assistant",
                        "content": full_response,
                        "timestamp": datetime.now().isoformat()
                    }
                    thread_history[thread_id].append(ai_message)

                # å‘é€å®Œæˆä¿¡å·
                yield f"data: {json.dumps({'type': 'done'})}\n\n"

            except Exception as e:
                print(f"âŒ Stream error: {e}")
                error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
                yield f"data: {json.dumps({'type': 'error', 'content': error_msg})}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )

    except Exception as e:
        print(f"âŒ Stream setup error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/runs/stream")
async def runs_stream(request: Request):
    """LangGraph SDK å…¼å®¹çš„æµå¼ç«¯ç‚¹"""
    try:
        # è·å–åŸå§‹è¯·æ±‚ä½“
        body = await request.body()
        print(f"ğŸ¤– LangGraph SDK runs/stream called with body: {body.decode()[:200]}...")

        # è§£æ JSON
        try:
            request_data = json.loads(body.decode())
        except json.JSONDecodeError as e:
            print(f"âŒ JSON decode error: {e}")
            raise HTTPException(status_code=400, detail="Invalid JSON")

        # ä»è¯·æ±‚ä¸­æå–æ¶ˆæ¯ - æ”¯æŒå¤šç§æ ¼å¼
        input_data = request_data.get("input", {})
        messages_data = input_data.get("messages", [])

        # å¦‚æœæ²¡æœ‰ input.messagesï¼Œå°è¯•ç›´æ¥ä» messages è·å–
        if not messages_data:
            messages_data = request_data.get("messages", [])

        print(f"ğŸ“ Extracted messages: {messages_data}")

        # è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼
        messages = []
        for msg_data in messages_data:
            if isinstance(msg_data, dict):
                if msg_data.get("type") == "human":
                    content = msg_data.get("content", "")
                    if isinstance(content, list):
                        # å¤„ç†å¤æ‚å†…å®¹æ ¼å¼
                        text_content = ""
                        for item in content:
                            if isinstance(item, dict) and item.get("type") == "text":
                                text_content += item.get("text", "")
                        messages.append(HumanMessage(content=text_content))
                    else:
                        messages.append(HumanMessage(content=str(content)))
                elif msg_data.get("type") == "ai":
                    messages.append(AIMessage(content=str(msg_data.get("content", ""))))

        print(f"ğŸ”„ Converted to {len(messages)} LangChain messages")

        # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼Œè¿”å›é”™è¯¯
        if not messages:
            print("âŒ No valid messages found")
            raise HTTPException(status_code=400, detail="No messages provided")

        # ç”Ÿæˆçº¿ç¨‹ID
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}

        # åˆå§‹åŒ–çº¿ç¨‹å†å²
        if thread_id not in thread_history:
            thread_history[thread_id] = []

        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        for msg in messages:
            if isinstance(msg, HumanMessage):
                thread_history[thread_id].append({
                    "role": "user",
                    "content": msg.content,
                    "timestamp": datetime.now().isoformat()
                })

        async def generate():
            full_response = ""
            chunk_count = 0
            try:
                print(f"ğŸŒŠ å¼€å§‹æµå¼å¤„ç†ï¼Œçº¿ç¨‹ID: {thread_id}")
                # LangGraph SDK æ ¼å¼çš„æµå¼å“åº”
                async for event in graph.astream_events(
                    {"messages": messages},
                    config,
                    version="v2"
                ):
                    kind = event["event"]
                    if kind == "on_chat_model_stream":
                        chunk_data = event["data"]
                        if "chunk" in chunk_data and hasattr(chunk_data["chunk"], "content"):
                            chunk_content = chunk_data["chunk"].content
                            if chunk_content:
                                chunk_count += 1
                                full_response += chunk_content
                                print(f"ğŸ“¦ æ”¶åˆ°chunk #{chunk_count}: {chunk_content[:20]}...")
                                # ä½¿ç”¨ LangGraph SDK å…¼å®¹çš„æ ¼å¼
                                yield f"data: {json.dumps({'event': 'values', 'data': {'messages': [{'type': 'ai', 'content': chunk_content}]}})}\n\n"

                print(f"âœ… æµå¼å¤„ç†å®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunksï¼Œæ€»é•¿åº¦: {len(full_response)}")

            except Exception as e:
                print(f"âŒ LangGraph SDK stream error: {e}")
                import traceback
                traceback.print_exc()
                yield f"data: {json.dumps({'event': 'error', 'data': {'error': str(e)}})}\n\n"
            finally:
                # ä¿å­˜AIå›å¤åˆ°å†å²ï¼ˆå³ä½¿æµå¼è¯·æ±‚è¢«ä¸­æ–­ä¹Ÿè¦ä¿å­˜ï¼‰
                if full_response:
                    thread_history[thread_id].append({
                        "role": "assistant",
                        "content": full_response,
                        "timestamp": datetime.now().isoformat()
                    })
                    print(f"ğŸ’¾ å·²ä¿å­˜å¯¹è¯åˆ°çº¿ç¨‹: {thread_id}, å…± {len(thread_history[thread_id])} æ¡æ¶ˆæ¯")

        return StreamingResponse(generate(), media_type="text/plain")

    except Exception as e:
        print(f"âŒ LangGraph SDK æµå¼å¤„ç†é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    print("ğŸš€ Starting LangGraph Chat Server...")
    print("ğŸ“š Based on LangGraph tutorials")
    print("ğŸŒŠ Real streaming with astream_events")
    uvicorn.run(app, host="0.0.0.0", port=2024)
