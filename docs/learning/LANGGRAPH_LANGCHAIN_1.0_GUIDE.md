# LangGraph & LangChain 1.0 å­¦ä¹ æŒ‡å—

## ğŸ“š æ¦‚è¿°

æœ¬æ–‡æ¡£åŸºäºå¯¹ LangGraph å’Œ LangChain 1.0 æºç çš„æ·±å…¥ç ”ç©¶ï¼Œæä¾›æœ€æ–°ç‰ˆæœ¬çš„æ ¸å¿ƒæ¦‚å¿µã€API ä½¿ç”¨å’Œæœ€ä½³å®è·µã€‚

### ç‰ˆæœ¬ä¿¡æ¯

- **LangGraph**: v0.6.8
- **LangChain Core**: v1.0.0
- **LangChain Classic**: v1.0.0

---

## ğŸ¯ LangGraph æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ

LangGraph æ˜¯ä¸€ä¸ª**ä½çº§ç¼–æ’æ¡†æ¶**ï¼Œç”¨äºæ„å»ºã€ç®¡ç†å’Œéƒ¨ç½²**é•¿æ—¶é—´è¿è¡Œçš„ã€æœ‰çŠ¶æ€çš„ Agent**ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
1. **æŒä¹…åŒ–æ‰§è¡Œ** - Agent å¯ä»¥ä»å¤±è´¥ä¸­æ¢å¤ï¼Œé•¿æ—¶é—´è¿è¡Œ
2. **Human-in-the-Loop** - åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ£€æŸ¥å’Œä¿®æ”¹ Agent çŠ¶æ€
3. **å…¨é¢çš„è®°å¿†** - çŸ­æœŸå·¥ä½œè®°å¿† + é•¿æœŸæŒä¹…åŒ–è®°å¿†
4. **è°ƒè¯•å·¥å…·** - ä¸ LangSmith é›†æˆï¼Œå¯è§†åŒ–æ‰§è¡Œè·¯å¾„
5. **ç”Ÿäº§å°±ç»ª** - å¯æ‰©å±•çš„åŸºç¡€è®¾æ–½

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…

```bash
# å®‰è£… LangGraph
pip install -U langgraph

# å®‰è£… LangChainï¼ˆå¯é€‰ï¼Œç”¨äºé›†æˆï¼‰
pip install -U langchain langchain-core

# å®‰è£…æ¨¡å‹æä¾›å•†ï¼ˆæ ¹æ®éœ€è¦é€‰æ‹©ï¼‰
pip install langchain-anthropic  # Anthropic
pip install langchain-openai     # OpenAI
pip install langchain-deepseek   # DeepSeek
```

### 2. ä½¿ç”¨é¢„æ„å»º Agentï¼ˆæœ€ç®€å•ï¼‰

```python
from langgraph.prebuilt import create_react_agent

# å®šä¹‰å·¥å…·
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”"""
    return f"{city} æ€»æ˜¯é˜³å…‰æ˜åªšï¼"

# åˆ›å»º Agent
agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"
)

# è¿è¡Œ Agent
response = agent.invoke({
    "messages": [{"role": "user", "content": "æ—§é‡‘å±±çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]
})
```

### 3. æ„å»ºè‡ªå®šä¹‰ Agentï¼ˆå®Œå…¨æ§åˆ¶ï¼‰

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# 1. å®šä¹‰çŠ¶æ€
class State(TypedDict):
    messages: Annotated[list, add_messages]

# 2. åˆ›å»ºå›¾æ„å»ºå™¨
graph_builder = StateGraph(State)

# 3. å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def chatbot(state: State):
    from langchain.chat_models import init_chat_model
    llm = init_chat_model("anthropic:claude-3-7-sonnet-latest")
    return {"messages": [llm.invoke(state["messages"])]}

# 4. æ·»åŠ èŠ‚ç‚¹
graph_builder.add_node("chatbot", chatbot)

# 5. æ·»åŠ è¾¹
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# 6. ç¼–è¯‘å›¾
graph = graph_builder.compile()

# 7. è¿è¡Œ
response = graph.invoke({
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼"}]
})
```

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### StateGraph - çŠ¶æ€å›¾

**StateGraph** æ˜¯ LangGraph çš„æ ¸å¿ƒç±»ï¼Œç”¨äºå®šä¹‰çŠ¶æ€æœºã€‚

```python
from langgraph.graph import StateGraph, START, END

# åˆ›å»ºçŠ¶æ€å›¾
graph = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("node_name", node_function)

# æ·»åŠ è¾¹ï¼ˆå›ºå®šè·¯å¾„ï¼‰
graph.add_edge(START, "node_name")
graph.add_edge("node_name", END)

# æ·»åŠ æ¡ä»¶è¾¹ï¼ˆåŠ¨æ€è·¯ç”±ï¼‰
graph.add_conditional_edges(
    "node_name",
    routing_function,
    {
        "path1": "next_node1",
        "path2": "next_node2",
    }
)

# ç¼–è¯‘
compiled_graph = graph.compile()
```

### State - çŠ¶æ€å®šä¹‰

**State** å®šä¹‰äº†å›¾çš„æ•°æ®ç»“æ„å’Œæ›´æ–°è§„åˆ™ã€‚

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class State(TypedDict):
    # ä½¿ç”¨ add_messages reducerï¼Œæ¶ˆæ¯ä¼šè¢«è¿½åŠ è€Œä¸æ˜¯è¦†ç›–
    messages: Annotated[list, add_messages]
    
    # æ™®é€šå­—æ®µï¼Œä¼šè¢«è¦†ç›–
    user_info: dict
    
    # è‡ªå®šä¹‰ reducer
    counter: Annotated[int, lambda x, y: x + y]
```

**Reducer å‡½æ•°**ï¼š
- `add_messages` - è¿½åŠ æ¶ˆæ¯åˆ°åˆ—è¡¨
- è‡ªå®šä¹‰å‡½æ•° - å®šä¹‰å¦‚ä½•åˆå¹¶çŠ¶æ€æ›´æ–°

### Node - èŠ‚ç‚¹

**Node** æ˜¯æ‰§è¡Œå•å…ƒï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå‡½æ•°ã€‚

```python
def my_node(state: State) -> dict:
    """
    èŠ‚ç‚¹å‡½æ•°æ¥æ”¶å½“å‰çŠ¶æ€ï¼Œè¿”å›çŠ¶æ€æ›´æ–°
    """
    # å¤„ç†é€»è¾‘
    result = process(state)
    
    # è¿”å›çŠ¶æ€æ›´æ–°ï¼ˆéƒ¨åˆ†æ›´æ–°ï¼‰
    return {
        "messages": [result],
        "counter": 1
    }
```

### Edge - è¾¹

**Edge** å®šä¹‰èŠ‚ç‚¹ä¹‹é—´çš„è½¬æ¢ã€‚

```python
# å›ºå®šè¾¹
graph.add_edge("node1", "node2")

# æ¡ä»¶è¾¹
def route(state: State) -> str:
    if state["counter"] > 10:
        return "end"
    return "continue"

graph.add_conditional_edges(
    "node1",
    route,
    {
        "end": END,
        "continue": "node2"
    }
)
```

---

## ğŸ’¾ æŒä¹…åŒ–å’Œè®°å¿†

### Checkpointer - æ£€æŸ¥ç‚¹

**Checkpointer** ç”¨äºä¿å­˜å’Œæ¢å¤å›¾çš„çŠ¶æ€ã€‚

```python
from langgraph.checkpoint.sqlite import SqliteSaver

# åˆ›å»º SQLite checkpointer
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

# ç¼–è¯‘æ—¶ä¼ å…¥
graph = graph_builder.compile(checkpointer=checkpointer)

# ä½¿ç”¨çº¿ç¨‹ ID è¿è¡Œ
config = {"configurable": {"thread_id": "conversation-1"}}
response = graph.invoke(input_data, config=config)
```

**æ”¯æŒçš„ Checkpointer**ï¼š
- `MemorySaver` - å†…å­˜å­˜å‚¨ï¼ˆæµ‹è¯•ç”¨ï¼‰
- `SqliteSaver` - SQLite æ•°æ®åº“
- `PostgresSaver` - PostgreSQL æ•°æ®åº“
- `RedisSaver` - Redis

### å¯¹è¯å†å²ç®¡ç†

```python
from langgraph.checkpoint.sqlite import SqliteSaver

checkpointer = SqliteSaver.from_conn_string("memory.db")
graph = create_react_agent(
    model=llm,
    tools=tools,
    checkpointer=checkpointer
)

# ç¬¬ä¸€è½®å¯¹è¯
config = {"configurable": {"thread_id": "user-123"}}
graph.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å«å¼ ä¸‰"}]},
    config=config
)

# ç¬¬äºŒè½®å¯¹è¯ï¼ˆè®°ä½ä¸Šä¸‹æ–‡ï¼‰
graph.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"}]},
    config=config
)
```

---

## ğŸŒŠ æµå¼è¾“å‡º

### åŸºæœ¬æµå¼

```python
# æµå¼è¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹çš„ç»“æœ
for chunk in graph.stream(input_data):
    print(chunk)
```

### æµå¼äº‹ä»¶ï¼ˆæ¨èï¼‰

```python
# æµå¼è¾“å‡ºè¯¦ç»†äº‹ä»¶
async for event in graph.astream_events(input_data, version="v2"):
    kind = event["event"]
    
    if kind == "on_chat_model_stream":
        # LLM è¾“å‡ºçš„æ¯ä¸ª token
        content = event["data"]["chunk"].content
        if content:
            print(content, end="", flush=True)
    
    elif kind == "on_tool_start":
        # å·¥å…·è°ƒç”¨å¼€å§‹
        print(f"\nè°ƒç”¨å·¥å…·: {event['name']}")
    
    elif kind == "on_tool_end":
        # å·¥å…·è°ƒç”¨ç»“æŸ
        print(f"å·¥å…·ç»“æœ: {event['data']['output']}")
```

### SSE æ ¼å¼æµå¼ï¼ˆç”¨äº Webï¼‰

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    async def event_generator():
        async for event in graph.astream_events(
            {"messages": request.messages},
            config={"configurable": {"thread_id": request.thread_id}},
            version="v2"
        ):
            # å‘é€ SSE æ ¼å¼
            yield f"event: {event['event']}\n"
            yield f"data: {json.dumps(event['data'])}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )
```

---

## ğŸ› ï¸ å·¥å…·è°ƒç”¨

### å®šä¹‰å·¥å…·

```python
from langchain_core.tools import tool

@tool
def search(query: str) -> str:
    """æœç´¢äº’è”ç½‘"""
    return f"æœç´¢ç»“æœ: {query}"

@tool
def calculator(expression: str) -> float:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    return eval(expression)

tools = [search, calculator]
```

### ä½¿ç”¨å·¥å…·

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=tools
)

response = agent.invoke({
    "messages": [{"role": "user", "content": "æœç´¢ LangGraph å¹¶è®¡ç®— 2+2"}]
})
```

---

## ğŸ”„ Human-in-the-Loop

### ä¸­æ–­å’Œæ‰¹å‡†

```python
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, START, END

# åˆ›å»ºå¸¦æ£€æŸ¥ç‚¹çš„å›¾
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

def human_approval_node(state: State):
    # è¿™ä¸ªèŠ‚ç‚¹ä¼šä¸­æ–­æ‰§è¡Œï¼Œç­‰å¾…äººå·¥æ‰¹å‡†
    return state

graph = StateGraph(State)
graph.add_node("process", process_node)
graph.add_node("approval", human_approval_node)
graph.add_node("execute", execute_node)

graph.add_edge(START, "process")
graph.add_edge("process", "approval")
graph.add_edge("approval", "execute")
graph.add_edge("execute", END)

# ç¼–è¯‘æ—¶è®¾ç½®ä¸­æ–­ç‚¹
compiled = graph.compile(
    checkpointer=checkpointer,
    interrupt_before=["approval"]  # åœ¨ approval èŠ‚ç‚¹å‰ä¸­æ–­
)

# ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆä¼šåœ¨ approval å‰åœæ­¢ï¼‰
config = {"configurable": {"thread_id": "thread-1"}}
result = compiled.invoke(input_data, config=config)

# äººå·¥å®¡æ ¸åç»§ç»­
# è·å–å½“å‰çŠ¶æ€
state = compiled.get_state(config)

# ä¿®æ”¹çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
compiled.update_state(config, {"approved": True})

# ç»§ç»­æ‰§è¡Œ
result = compiled.invoke(None, config=config)
```

---

## ğŸ“Š å¤š Agent åä½œ

### Supervisor æ¨¡å¼

```python
from langgraph.prebuilt import create_react_agent
from typing import Literal

# åˆ›å»ºä¸“é—¨çš„ Agent
researcher = create_react_agent(model=llm, tools=[search_tool])
writer = create_react_agent(model=llm, tools=[write_tool])

# Supervisor è·¯ç”±å‡½æ•°
def supervisor_router(state: State) -> Literal["researcher", "writer", "end"]:
    messages = state["messages"]
    last_message = messages[-1]
    
    if "research" in last_message.content.lower():
        return "researcher"
    elif "write" in last_message.content.lower():
        return "writer"
    else:
        return "end"

# æ„å»º Supervisor å›¾
graph = StateGraph(State)
graph.add_node("researcher", researcher)
graph.add_node("writer", writer)

graph.add_conditional_edges(
    START,
    supervisor_router,
    {
        "researcher": "researcher",
        "writer": "writer",
        "end": END
    }
)

supervisor = graph.compile()
```

---

## ğŸ¨ LangChain 1.0 æ ¸å¿ƒæ¦‚å¿µ

### Runnable æ¥å£

**Runnable** æ˜¯ LangChain çš„æ ¸å¿ƒæŠ½è±¡ï¼Œæ‰€æœ‰ç»„ä»¶éƒ½å®ç°è¿™ä¸ªæ¥å£ã€‚

```python
from langchain_core.runnables import Runnable

# æ‰€æœ‰ Runnable éƒ½æœ‰è¿™äº›æ–¹æ³•ï¼š
result = runnable.invoke(input)           # åŒæ­¥è°ƒç”¨
result = await runnable.ainvoke(input)    # å¼‚æ­¥è°ƒç”¨
for chunk in runnable.stream(input):      # æµå¼è¾“å‡º
    print(chunk)
```

### LCEL - LangChain Expression Language

**LCEL** æ˜¯ç”¨äºç»„åˆ Runnable çš„å£°æ˜å¼è¯­è¨€ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chat_models import init_chat_model

# åˆ›å»ºç»„ä»¶
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"),
    ("user", "{input}")
])
model = init_chat_model("anthropic:claude-3-7-sonnet-latest")
output_parser = StrOutputParser()

# ä½¿ç”¨ | æ“ä½œç¬¦ç»„åˆ
chain = prompt | model | output_parser

# è°ƒç”¨
result = chain.invoke({"input": "ä½ å¥½"})
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. çŠ¶æ€è®¾è®¡

```python
# âœ… å¥½çš„è®¾è®¡
class State(TypedDict):
    messages: Annotated[list, add_messages]  # ä½¿ç”¨ reducer
    user_id: str                              # ç®€å•å­—æ®µ
    metadata: dict                            # ç»“æ„åŒ–æ•°æ®

# âŒ ä¸å¥½çš„è®¾è®¡
class State(TypedDict):
    everything: dict  # å¤ªå®½æ³›ï¼Œéš¾ä»¥ç»´æŠ¤
```

### 2. èŠ‚ç‚¹å‡½æ•°

```python
# âœ… å¥½çš„èŠ‚ç‚¹å‡½æ•°
def my_node(state: State) -> dict:
    """æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²"""
    # å•ä¸€èŒè´£
    result = process_data(state["messages"])
    # è¿”å›éƒ¨åˆ†æ›´æ–°
    return {"messages": [result]}

# âŒ ä¸å¥½çš„èŠ‚ç‚¹å‡½æ•°
def my_node(state: State) -> dict:
    # åšå¤ªå¤šäº‹æƒ…
    # æ²¡æœ‰æ–‡æ¡£
    # ä¿®æ”¹å…¨å±€çŠ¶æ€
    pass
```

### 3. é”™è¯¯å¤„ç†

```python
def safe_node(state: State) -> dict:
    try:
        result = risky_operation(state)
        return {"messages": [result]}
    except Exception as e:
        # è®°å½•é”™è¯¯
        logger.error(f"èŠ‚ç‚¹å¤±è´¥: {e}")
        # è¿”å›é”™è¯¯æ¶ˆæ¯
        return {
            "messages": [{"role": "assistant", "content": f"æŠ±æ­‰ï¼Œå‘ç”Ÿé”™è¯¯: {e}"}]
        }
```

### 4. æµ‹è¯•

```python
import pytest

def test_my_node():
    # å‡†å¤‡æµ‹è¯•çŠ¶æ€
    state = {
        "messages": [{"role": "user", "content": "æµ‹è¯•"}]
    }
    
    # è°ƒç”¨èŠ‚ç‚¹
    result = my_node(state)
    
    # æ–­è¨€
    assert "messages" in result
    assert len(result["messages"]) > 0
```

---

## ğŸ”— ç›¸å…³èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [LangGraph æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [LangChain æ–‡æ¡£](https://python.langchain.com/)
- [LangSmith](https://smith.langchain.com/)

### æœ¬åœ°æºç 
- LangGraph: `/Users/kay/code/github/agent-chat-ui/langgraph/`
- LangChain: `/Users/kay/code/github/agent-chat-ui/langchain/`

### æ ¸å¿ƒæ¨¡å—
- `langgraph/libs/langgraph/langgraph/graph/` - å›¾æ„å»º
- `langgraph/libs/langgraph/langgraph/pregel/` - æ‰§è¡Œå¼•æ“
- `langgraph/libs/langgraph/langgraph/checkpoint/` - æŒä¹…åŒ–
- `langchain/libs/core/langchain_core/runnables/` - Runnable æ¥å£

---

## ğŸ“ é«˜çº§ä¸»é¢˜

### å­å›¾ (Subgraphs)

å­å›¾å…è®¸ä½ å°†å¤æ‚çš„å›¾åˆ†è§£ä¸ºå¯é‡ç”¨çš„ç»„ä»¶ã€‚

```python
from langgraph.graph import StateGraph, START, END

# å®šä¹‰å­å›¾çŠ¶æ€
class SubState(TypedDict):
    data: str

# åˆ›å»ºå­å›¾
sub_graph = StateGraph(SubState)
sub_graph.add_node("process", lambda s: {"data": s["data"].upper()})
sub_graph.add_edge(START, "process")
sub_graph.add_edge("process", END)
compiled_sub = sub_graph.compile()

# åœ¨ä¸»å›¾ä¸­ä½¿ç”¨å­å›¾
class MainState(TypedDict):
    messages: Annotated[list, add_messages]
    processed_data: str

main_graph = StateGraph(MainState)

def use_subgraph(state: MainState):
    # è°ƒç”¨å­å›¾
    result = compiled_sub.invoke({"data": state["messages"][-1].content})
    return {"processed_data": result["data"]}

main_graph.add_node("subgraph_node", use_subgraph)
main_graph.add_edge(START, "subgraph_node")
main_graph.add_edge("subgraph_node", END)

main = main_graph.compile()
```

### åŠ¨æ€å›¾ (Dynamic Graphs)

ä½¿ç”¨ `Send` å®ç°åŠ¨æ€å¹¶è¡Œæ‰§è¡Œã€‚

```python
from langgraph.types import Send

def fan_out_node(state: State):
    # åŠ¨æ€åˆ›å»ºå¤šä¸ªå¹¶è¡Œä»»åŠ¡
    tasks = state["tasks"]
    return [
        Send("process_task", {"task": task})
        for task in tasks
    ]

def process_task(state: dict):
    # å¤„ç†å•ä¸ªä»»åŠ¡
    result = process(state["task"])
    return {"results": [result]}

graph = StateGraph(State)
graph.add_node("fan_out", fan_out_node)
graph.add_node("process_task", process_task)
graph.add_node("collect", collect_results)

graph.add_conditional_edges(
    "fan_out",
    lambda s: [Send("process_task", {"task": t}) for t in s["tasks"]]
)
graph.add_edge("process_task", "collect")
```

### é‡è¯•ç­–ç•¥

```python
from langgraph.types import RetryPolicy

# å®šä¹‰é‡è¯•ç­–ç•¥
retry_policy = RetryPolicy(
    max_attempts=3,
    backoff_factor=2.0,
    retry_on=[Exception]
)

# åœ¨èŠ‚ç‚¹ä¸Šåº”ç”¨
graph.add_node(
    "risky_node",
    risky_function,
    retry=retry_policy
)
```

### ç¼“å­˜ç­–ç•¥

```python
from langgraph.types import CachePolicy

# å®šä¹‰ç¼“å­˜ç­–ç•¥
cache_policy = CachePolicy(
    ttl=3600,  # 1å°æ—¶
    key=lambda state: state["user_id"]
)

# åœ¨èŠ‚ç‚¹ä¸Šåº”ç”¨
graph.add_node(
    "cached_node",
    expensive_function,
    cache=cache_policy
)
```

---

## ğŸ” è°ƒè¯•å’Œç›‘æ§

### ä½¿ç”¨ LangSmith

```python
import os

# è®¾ç½® LangSmith
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-project"

# æ­£å¸¸è¿è¡Œï¼Œè‡ªåŠ¨è¿½è¸ª
response = graph.invoke(input_data)
```

### å¯è§†åŒ–å›¾ç»“æ„

```python
from IPython.display import Image, display

# ç”Ÿæˆå›¾çš„å¯è§†åŒ–
display(Image(graph.get_graph().draw_mermaid_png()))
```

### æ‰“å°è°ƒè¯•ä¿¡æ¯

```python
def debug_node(state: State):
    print(f"å½“å‰çŠ¶æ€: {state}")
    result = process(state)
    print(f"å¤„ç†ç»“æœ: {result}")
    return result

graph.add_node("debug", debug_node)
```

---

## ğŸŒ å®é™…åº”ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: å¸¦è®°å¿†çš„èŠå¤©æœºå™¨äºº

```python
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain.chat_models import init_chat_model

# åˆ›å»º checkpointer
checkpointer = SqliteSaver.from_conn_string("chat_memory.db")

# åˆ›å»ºæ¨¡å‹
llm = init_chat_model("deepseek:deepseek-chat", temperature=0.7)

# åˆ›å»º agent
agent = create_react_agent(
    model=llm,
    tools=[],
    checkpointer=checkpointer
)

# å¯¹è¯ 1
config = {"configurable": {"thread_id": "user-123"}}
response1 = agent.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å«å¼ ä¸‰ï¼Œæˆ‘å–œæ¬¢ç¼–ç¨‹"}]},
    config=config
)

# å¯¹è¯ 2ï¼ˆè®°ä½ä¸Šä¸‹æ–‡ï¼‰
response2 = agent.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿæˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ"}]},
    config=config
)
```

### ç¤ºä¾‹ 2: RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)

```python
from langgraph.graph import StateGraph, START, END
from langchain_core.documents import Document

class RAGState(TypedDict):
    question: str
    documents: list[Document]
    answer: str

def retrieve(state: RAGState):
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    docs = vector_store.similarity_search(state["question"], k=3)
    return {"documents": docs}

def generate(state: RAGState):
    # åŸºäºæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
    context = "\n".join([doc.page_content for doc in state["documents"]])
    prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {state['question']}"
    answer = llm.invoke(prompt)
    return {"answer": answer.content}

# æ„å»º RAG å›¾
rag_graph = StateGraph(RAGState)
rag_graph.add_node("retrieve", retrieve)
rag_graph.add_node("generate", generate)
rag_graph.add_edge(START, "retrieve")
rag_graph.add_edge("retrieve", "generate")
rag_graph.add_edge("generate", END)

rag = rag_graph.compile()

# ä½¿ç”¨
result = rag.invoke({"question": "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ"})
print(result["answer"])
```

### ç¤ºä¾‹ 3: å¤šæ­¥éª¤å·¥ä½œæµ

```python
class WorkflowState(TypedDict):
    input: str
    plan: str
    research: str
    draft: str
    final: str

def plan_step(state: WorkflowState):
    # åˆ¶å®šè®¡åˆ’
    plan = llm.invoke(f"ä¸ºä»¥ä¸‹ä»»åŠ¡åˆ¶å®šè®¡åˆ’: {state['input']}")
    return {"plan": plan.content}

def research_step(state: WorkflowState):
    # ç ”ç©¶
    research = search_tool.invoke(state["plan"])
    return {"research": research}

def draft_step(state: WorkflowState):
    # èµ·è‰
    draft = llm.invoke(f"åŸºäºç ”ç©¶ç»“æœèµ·è‰: {state['research']}")
    return {"draft": draft.content}

def review_step(state: WorkflowState):
    # å®¡æ ¸å’Œå®Œå–„
    final = llm.invoke(f"å®¡æ ¸å¹¶å®Œå–„è‰ç¨¿: {state['draft']}")
    return {"final": final.content}

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(WorkflowState)
workflow.add_node("plan", plan_step)
workflow.add_node("research", research_step)
workflow.add_node("draft", draft_step)
workflow.add_node("review", review_step)

workflow.add_edge(START, "plan")
workflow.add_edge("plan", "research")
workflow.add_edge("research", "draft")
workflow.add_edge("draft", "review")
workflow.add_edge("review", END)

app = workflow.compile()

# è¿è¡Œ
result = app.invoke({"input": "å†™ä¸€ç¯‡å…³äº AI çš„æ–‡ç« "})
print(result["final"])
```

---

## ğŸ“š å¸¸è§æ¨¡å¼

### 1. ReAct Agent æ¨¡å¼

```python
from langgraph.prebuilt import create_react_agent

# ReAct = Reasoning + Acting
# Agent ä¼šï¼šæ€è€ƒ -> è¡ŒåŠ¨ -> è§‚å¯Ÿ -> é‡å¤

agent = create_react_agent(
    model=llm,
    tools=[search, calculator, weather],
    prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œä½¿ç”¨å·¥å…·æ¥å›ç­”é—®é¢˜"
)
```

### 2. Plan-and-Execute æ¨¡å¼

```python
def planner(state):
    # åˆ¶å®šè®¡åˆ’
    plan = llm.invoke(f"ä¸ºä»¥ä¸‹ç›®æ ‡åˆ¶å®šæ­¥éª¤: {state['goal']}")
    return {"plan": plan.content.split("\n")}

def executor(state):
    # æ‰§è¡Œæ¯ä¸ªæ­¥éª¤
    results = []
    for step in state["plan"]:
        result = execute_step(step)
        results.append(result)
    return {"results": results}

graph = StateGraph(State)
graph.add_node("planner", planner)
graph.add_node("executor", executor)
graph.add_edge(START, "planner")
graph.add_edge("planner", "executor")
graph.add_edge("executor", END)
```

### 3. Reflection æ¨¡å¼

```python
def generate(state):
    # ç”Ÿæˆåˆå§‹ç­”æ¡ˆ
    answer = llm.invoke(state["question"])
    return {"answer": answer.content}

def reflect(state):
    # åæ€ç­”æ¡ˆè´¨é‡
    reflection = llm.invoke(f"è¯„ä¼°è¿™ä¸ªç­”æ¡ˆ: {state['answer']}")
    return {"reflection": reflection.content}

def should_continue(state):
    # å†³å®šæ˜¯å¦éœ€è¦æ”¹è¿›
    if "good" in state["reflection"].lower():
        return "end"
    return "generate"

graph = StateGraph(State)
graph.add_node("generate", generate)
graph.add_node("reflect", reflect)

graph.add_edge(START, "generate")
graph.add_edge("generate", "reflect")
graph.add_conditional_edges(
    "reflect",
    should_continue,
    {"end": END, "generate": "generate"}
)
```

---

## âš ï¸ å¸¸è§é™·é˜±

### 1. çŠ¶æ€æ›´æ–°è¦†ç›–

```python
# âŒ é”™è¯¯ï¼šä¼šè¦†ç›–æ•´ä¸ª messages åˆ—è¡¨
def bad_node(state: State):
    return {"messages": [new_message]}

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ add_messages reducer
class State(TypedDict):
    messages: Annotated[list, add_messages]

def good_node(state: State):
    return {"messages": [new_message]}  # ä¼šè¿½åŠ ï¼Œä¸ä¼šè¦†ç›–
```

### 2. å¿˜è®°ç¼–è¯‘å›¾

```python
# âŒ é”™è¯¯
graph = StateGraph(State)
graph.add_node("node", func)
result = graph.invoke(input)  # é”™è¯¯ï¼

# âœ… æ­£ç¡®
graph = StateGraph(State)
graph.add_node("node", func)
compiled = graph.compile()  # å¿…é¡»ç¼–è¯‘
result = compiled.invoke(input)
```

### 3. å¾ªç¯ä¾èµ–

```python
# âŒ é”™è¯¯ï¼šåˆ›å»ºäº†æ— é™å¾ªç¯
graph.add_edge("node1", "node2")
graph.add_edge("node2", "node1")  # æ­»å¾ªç¯ï¼

# âœ… æ­£ç¡®ï¼šä½¿ç”¨æ¡ä»¶è¾¹å’Œç»ˆæ­¢æ¡ä»¶
def should_continue(state):
    if state["counter"] > 10:
        return "end"
    return "continue"

graph.add_conditional_edges(
    "node1",
    should_continue,
    {"end": END, "continue": "node2"}
)
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. ä½¿ç”¨å¼‚æ­¥

```python
# åŒæ­¥ç‰ˆæœ¬
result = graph.invoke(input)

# å¼‚æ­¥ç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼‰
result = await graph.ainvoke(input)
```

### 2. æ‰¹å¤„ç†

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥
inputs = [input1, input2, input3]
results = graph.batch(inputs)
```

### 3. å¹¶è¡Œæ‰§è¡Œ

```python
from langgraph.types import Send

def parallel_node(state):
    # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
    return [
        Send("task1", data1),
        Send("task2", data2),
        Send("task3", data3),
    ]
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-01-16
**åŸºäºæºç ç‰ˆæœ¬**: LangGraph 0.6.8, LangChain 1.0.0
