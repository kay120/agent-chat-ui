# LangGraph å¿«é€Ÿå‚è€ƒ

## ğŸ“¦ å®‰è£…

```bash
pip install -U langgraph langchain langchain-deepseek
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### é¢„æ„å»º Agent

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="deepseek:deepseek-chat",
    tools=[tool1, tool2],
    prompt="ç³»ç»Ÿæç¤º"
)

response = agent.invoke({"messages": [{"role": "user", "content": "é—®é¢˜"}]})
```

### è‡ªå®šä¹‰ Agent

```python
from langgraph.graph import StateGraph, START, END
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

# 1. å®šä¹‰çŠ¶æ€
class State(TypedDict):
    messages: Annotated[list, add_messages]

# 2. åˆ›å»ºå›¾
graph = StateGraph(State)

# 3. æ·»åŠ èŠ‚ç‚¹
def my_node(state: State):
    return {"messages": [llm.invoke(state["messages"])]}

graph.add_node("node_name", my_node)

# 4. æ·»åŠ è¾¹
graph.add_edge(START, "node_name")
graph.add_edge("node_name", END)

# 5. ç¼–è¯‘
app = graph.compile()

# 6. è¿è¡Œ
result = app.invoke({"messages": [{"role": "user", "content": "ä½ å¥½"}]})
```

## ğŸ—ï¸ æ ¸å¿ƒ API

### StateGraph

```python
from langgraph.graph import StateGraph, START, END

graph = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("name", function)

# æ·»åŠ å›ºå®šè¾¹
graph.add_edge(START, "node1")
graph.add_edge("node1", "node2")
graph.add_edge("node2", END)

# æ·»åŠ æ¡ä»¶è¾¹
graph.add_conditional_edges(
    "node1",
    routing_function,
    {"path1": "node2", "path2": END}
)

# ç¼–è¯‘
app = graph.compile()
```

### State å®šä¹‰

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class State(TypedDict):
    # ä½¿ç”¨ reducerï¼ˆè¿½åŠ ï¼‰
    messages: Annotated[list, add_messages]
    
    # æ™®é€šå­—æ®µï¼ˆè¦†ç›–ï¼‰
    user_id: str
    counter: int
    
    # è‡ªå®šä¹‰ reducer
    total: Annotated[int, lambda x, y: x + y]
```

### èŠ‚ç‚¹å‡½æ•°

```python
def my_node(state: State) -> dict:
    """
    æ¥æ”¶å½“å‰çŠ¶æ€ï¼Œè¿”å›çŠ¶æ€æ›´æ–°
    """
    # å¤„ç†é€»è¾‘
    result = process(state)
    
    # è¿”å›éƒ¨åˆ†æ›´æ–°
    return {
        "messages": [result],
        "counter": state["counter"] + 1
    }
```

## ğŸ’¾ æŒä¹…åŒ–

### SQLite Checkpointer

```python
from langgraph.checkpoint.sqlite import SqliteSaver

checkpointer = SqliteSaver.from_conn_string("memory.db")

app = graph.compile(checkpointer=checkpointer)

# ä½¿ç”¨çº¿ç¨‹ ID
config = {"configurable": {"thread_id": "user-123"}}
result = app.invoke(input_data, config=config)
```

### å†…å­˜ Checkpointer

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
app = graph.compile(checkpointer=checkpointer)
```

## ğŸŒŠ æµå¼è¾“å‡º

### åŸºæœ¬æµå¼

```python
for chunk in app.stream(input_data):
    print(chunk)
```

### æµå¼äº‹ä»¶

```python
async for event in app.astream_events(input_data, version="v2"):
    if event["event"] == "on_chat_model_stream":
        content = event["data"]["chunk"].content
        if content:
            print(content, end="", flush=True)
```

### SSE æ ¼å¼

```python
async def event_generator():
    async for event in app.astream_events(input_data, version="v2"):
        yield f"event: {event['event']}\n"
        yield f"data: {json.dumps(event['data'])}\n\n"
```

## ğŸ› ï¸ å·¥å…·

### å®šä¹‰å·¥å…·

```python
from langchain_core.tools import tool

@tool
def my_tool(param: str) -> str:
    """å·¥å…·æè¿°"""
    return f"ç»“æœ: {param}"

tools = [my_tool]
```

### ä½¿ç”¨å·¥å…·

```python
agent = create_react_agent(
    model=llm,
    tools=tools
)
```

## ğŸ”„ æ¡ä»¶è·¯ç”±

```python
def router(state: State) -> str:
    if condition:
        return "path1"
    return "path2"

graph.add_conditional_edges(
    "source_node",
    router,
    {
        "path1": "node1",
        "path2": "node2"
    }
)
```

## ğŸ¯ Human-in-the-Loop

```python
# ç¼–è¯‘æ—¶è®¾ç½®ä¸­æ–­ç‚¹
app = graph.compile(
    checkpointer=checkpointer,
    interrupt_before=["approval_node"]
)

# ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆä¼šä¸­æ–­ï¼‰
config = {"configurable": {"thread_id": "thread-1"}}
result = app.invoke(input_data, config=config)

# è·å–çŠ¶æ€
state = app.get_state(config)

# ä¿®æ”¹çŠ¶æ€
app.update_state(config, {"approved": True})

# ç»§ç»­æ‰§è¡Œ
result = app.invoke(None, config=config)
```

## ğŸ“Š å¤š Agent

### Supervisor æ¨¡å¼

```python
def supervisor(state: State) -> str:
    if "research" in state["messages"][-1].content:
        return "researcher"
    elif "write" in state["messages"][-1].content:
        return "writer"
    return "end"

graph.add_conditional_edges(
    START,
    supervisor,
    {
        "researcher": "researcher_node",
        "writer": "writer_node",
        "end": END
    }
)
```

## ğŸ” è°ƒè¯•

### LangSmith

```python
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-key"
os.environ["LANGCHAIN_PROJECT"] = "project-name"
```

### å¯è§†åŒ–

```python
from IPython.display import Image, display

display(Image(app.get_graph().draw_mermaid_png()))
```

## ğŸ“ å¸¸ç”¨æ¨¡å¼

### ReAct Agent

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(model=llm, tools=tools)
```

### RAG

```python
def retrieve(state):
    docs = vector_store.similarity_search(state["question"])
    return {"documents": docs}

def generate(state):
    context = "\n".join([d.page_content for d in state["documents"]])
    answer = llm.invoke(f"Context: {context}\n\nQuestion: {state['question']}")
    return {"answer": answer.content}

graph.add_node("retrieve", retrieve)
graph.add_node("generate", generate)
graph.add_edge(START, "retrieve")
graph.add_edge("retrieve", "generate")
graph.add_edge("generate", END)
```

### Plan-and-Execute

```python
def plan(state):
    plan = llm.invoke(f"åˆ¶å®šè®¡åˆ’: {state['goal']}")
    return {"plan": plan.content.split("\n")}

def execute(state):
    results = [execute_step(step) for step in state["plan"]]
    return {"results": results}

graph.add_edge(START, "plan")
graph.add_edge("plan", "execute")
graph.add_edge("execute", END)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å¼‚æ­¥

```python
# å¼‚æ­¥è°ƒç”¨
result = await app.ainvoke(input_data)

# å¼‚æ­¥æµå¼
async for chunk in app.astream(input_data):
    print(chunk)
```

### æ‰¹å¤„ç†

```python
results = app.batch([input1, input2, input3])
```

### å¹¶è¡Œæ‰§è¡Œ

```python
from langgraph.types import Send

def parallel(state):
    return [
        Send("task1", data1),
        Send("task2", data2),
    ]
```

## ğŸ¨ LangChain LCEL

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ç»„åˆé“¾
chain = prompt | model | output_parser

# è°ƒç”¨
result = chain.invoke({"input": "é—®é¢˜"})

# æµå¼
for chunk in chain.stream({"input": "é—®é¢˜"}):
    print(chunk, end="", flush=True)
```

## ğŸ“š å¸¸ç”¨å¯¼å…¥

```python
# LangGraph
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Send, RetryPolicy, CachePolicy

# LangChain
from langchain.chat_models import init_chat_model
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# ç±»å‹
from typing import Annotated
from typing_extensions import TypedDict
```

## ğŸ”— èµ„æº

- [LangGraph æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [LangChain æ–‡æ¡£](https://python.langchain.com/)
- [å®Œæ•´æŒ‡å—](./LANGGRAPH_LANGCHAIN_1.0_GUIDE.md)

---

**ç‰ˆæœ¬**: LangGraph 0.6.8, LangChain 1.0.0  
**æ›´æ–°**: 2025-01-16

