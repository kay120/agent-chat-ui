"""
LangGraph å›¾å®šä¹‰ - ç”¨äº langgraph_api
è¿™ä¸ªæ–‡ä»¶ä¸ä½¿ç”¨ç›¸å¯¹å¯¼å…¥ï¼Œå¯ä»¥è¢« langgraph_api ç›´æ¥åŠ è½½
"""
import os
import sqlite3
from typing import Annotated
from typing_extensions import TypedDict
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langgraph.graph import StateGraph, START, END, add_messages
from langgraph.checkpoint.sqlite import SqliteSaver  # ä½¿ç”¨ SQLite æŒä¹…åŒ–
from langchain_openai import ChatOpenAI


# å®šä¹‰çŠ¶æ€
class State(TypedDict):
    """å¯¹è¯çŠ¶æ€"""
    messages: Annotated[list[BaseMessage], add_messages]


# åˆå§‹åŒ– LLM
def get_llm():
    """è·å– LLM å®ä¾‹ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼‰"""
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY", "")
    deepseek_base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
    deepseek_model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
    max_tokens = int(os.getenv("DEEPSEEK_MAX_TOKENS", "8000"))
    temperature = float(os.getenv("DEEPSEEK_TEMPERATURE", "0.7"))

    if not deepseek_api_key:
        raise ValueError("DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

    llm = ChatOpenAI(
        model=deepseek_model,
        api_key=deepseek_api_key,
        base_url=deepseek_base_url,
        temperature=temperature,
        streaming=True,
        max_tokens=max_tokens
    )

    print(f"âœ… å·²åˆå§‹åŒ– DeepSeek æ¨¡å‹: {deepseek_model}")
    print(f"   - max_tokens: {max_tokens}")
    print(f"   - temperature: {temperature}")
    return llm


# åˆ›å»º LLM å®ä¾‹
llm = get_llm()


# å®šä¹‰èŠå¤©èŠ‚ç‚¹
def chatbot(state: State) -> State:
    """èŠå¤©èŠ‚ç‚¹"""
    total_messages = len(state['messages'])
    print(f"ğŸ¤– Chatbot node called with {total_messages} messages")

    # æ‰“å°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    if state['messages']:
        last_msg = state['messages'][-1]
        # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯å†…å®¹
        if isinstance(last_msg.content, str):
            content_preview = last_msg.content[:100]
        elif isinstance(last_msg.content, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼ˆå¤šæ¨¡æ€æ¶ˆæ¯ï¼‰ï¼Œæå–æ–‡æœ¬éƒ¨åˆ†
            text_parts = [item.get('text', '') for item in last_msg.content if isinstance(item, dict) and 'text' in item]
            content_preview = ' '.join(text_parts)[:100]
        else:
            content_preview = str(last_msg.content)[:100]
        print(f"ğŸ‘¤ ç”¨æˆ·æ¶ˆæ¯: {content_preview}...")

    # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šåªä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
    MAX_HISTORY_MESSAGES = 10  # æœ€å¤šä¿ç•™ 10 æ¡æ¶ˆæ¯ï¼ˆ5 è½®å¯¹è¯ï¼‰

    messages_to_send = state["messages"]
    if len(messages_to_send) > MAX_HISTORY_MESSAGES:
        # ä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯
        messages_to_send = messages_to_send[-MAX_HISTORY_MESSAGES:]
        print(f"âš¡ æ€§èƒ½ä¼˜åŒ–: é™åˆ¶ä¸Šä¸‹æ–‡ä¸ºæœ€è¿‘ {MAX_HISTORY_MESSAGES} æ¡æ¶ˆæ¯ï¼ˆæ€»å…± {total_messages} æ¡ï¼‰")

    # ä½¿ç”¨æµå¼è°ƒç”¨ LLMï¼Œå®æ—¶è¾“å‡º
    print(f"ğŸ”„ å¼€å§‹æµå¼è°ƒç”¨ LLMï¼ˆå‘é€ {len(messages_to_send)} æ¡æ¶ˆæ¯ï¼‰...")
    full_content = ""

    for chunk in llm.stream(messages_to_send):
        if chunk.content:
            full_content += chunk.content
            # å®æ—¶æ‰“å°æ¯ä¸ªå—ï¼ˆåªæ‰“å°æ–°å¢å†…å®¹ï¼‰
            print(chunk.content, end='', flush=True)

    print()  # æ¢è¡Œ
    print(f"âœ… æµå¼è¾“å‡ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")

    # åˆ›å»ºå®Œæ•´çš„å“åº”æ¶ˆæ¯
    from langchain_core.messages import AIMessage
    response = AIMessage(content=full_content)

    # æ‰“å° AI å›å¤é¢„è§ˆ
    reply_preview = full_content[:200] if len(full_content) > 200 else full_content
    print(f"ğŸ¤– AI å›å¤é¢„è§ˆ: {reply_preview}...")

    # è¿”å›æ–°çŠ¶æ€
    return {"messages": [response]}


# åˆ›å»ºå›¾
def create_graph():
    """åˆ›å»º LangGraphï¼ˆä½¿ç”¨ SQLite æŒä¹…åŒ–ï¼‰"""
    workflow = StateGraph(State)

    # æ·»åŠ èŠå¤©èŠ‚ç‚¹
    workflow.add_node("chatbot", chatbot)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("chatbot")

    # æ·»åŠ ç»“æŸè¾¹
    workflow.add_edge("chatbot", END)

    # ğŸ—„ï¸ ä½¿ç”¨ SQLite æŒä¹…åŒ–å­˜å‚¨
    db_path = os.getenv("SQLITE_DB_PATH", "checkpoints.sqlite")
    # check_same_thread=False å…è®¸å¤šçº¿ç¨‹è®¿é—®ï¼ˆSqliteSaver å†…éƒ¨æœ‰é”ä¿è¯çº¿ç¨‹å®‰å…¨ï¼‰
    conn = sqlite3.connect(db_path, check_same_thread=False)
    memory = SqliteSaver(conn)

    # ç¼–è¯‘å›¾ï¼ˆä½¿ç”¨ SQLite checkpointerï¼‰
    compiled_graph = workflow.compile(checkpointer=memory)

    print("âœ… LangGraph åˆ›å»ºå®Œæˆ")
    print("âœ… å·²å¯ç”¨ SQLite æŒä¹…åŒ–å­˜å‚¨")
    print(f"   - æ•°æ®åº“æ–‡ä»¶: {db_path}")
    print(f"   - é‡å¯åå¯¹è¯å†å²ä¸ä¼šä¸¢å¤±")
    return compiled_graph


# åˆ›å»ºå›¾å®ä¾‹ï¼ˆä¾› langgraph_api ä½¿ç”¨ï¼‰
graph = create_graph()

