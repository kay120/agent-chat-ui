#!/usr/bin/env python3
"""
LangGraph æœåŠ¡å™¨ - é›†æˆçœŸå®AIæ¨¡å‹
æ”¯æŒOpenAIã€Ollamaç­‰å¤šç§æ¨¡å‹æä¾›å•†
"""

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, List, Optional
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from langchain_core.messages import HumanMessage, AIMessage
from pydantic import BaseModel
import uuid
from datetime import datetime
import json
import asyncio
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ¨¡å‹é…ç½®
MODEL_PROVIDER = os.getenv("MODEL_PROVIDER", "deepseek")  # openai, deepseek, ollama, mock
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama2")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# åˆå§‹åŒ–æ¨¡å‹
llm = None

def init_model():
    global llm
    try:
        if MODEL_PROVIDER == "openai" and OPENAI_API_KEY:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(
                model=OPENAI_MODEL,
                temperature=0.7,
                streaming=True
            )
            # è®¾ç½® API key ä¸ºç¯å¢ƒå˜é‡
            os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
            print(f"âœ… å·²åˆå§‹åŒ– OpenAI æ¨¡å‹: {OPENAI_MODEL}")
        
        elif MODEL_PROVIDER == "deepseek" and DEEPSEEK_API_KEY:
            from langchain_openai import ChatOpenAI
            # è®¾ç½® DeepSeek API é…ç½®
            os.environ["OPENAI_API_KEY"] = DEEPSEEK_API_KEY
            os.environ["OPENAI_API_BASE"] = DEEPSEEK_BASE_URL
            llm = ChatOpenAI(
                model=DEEPSEEK_MODEL,
                temperature=0.7,
                streaming=True,
                base_url=DEEPSEEK_BASE_URL
            )
            print(f"âœ… å·²åˆå§‹åŒ– DeepSeek æ¨¡å‹: {DEEPSEEK_MODEL}")
        
        elif MODEL_PROVIDER == "ollama":
            try:
                from langchain_community.llms import Ollama
                from langchain_core.language_models.chat_models import BaseChatModel
                # ä½¿ç”¨ Ollama ä½œä¸ºåŸºç¡€æ¨¡å‹
                llm = Ollama(
                    model=OLLAMA_MODEL,
                    base_url=OLLAMA_BASE_URL,
                    temperature=0.7
                )
                print(f"âœ… å·²åˆå§‹åŒ– Ollama æ¨¡å‹: {OLLAMA_MODEL}")
            except ImportError:
                print(f"âš ï¸  Ollama å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                llm = None
        
        else:
            print(f"âš ï¸  æœªé…ç½®æœ‰æ•ˆçš„æ¨¡å‹æä¾›å•†ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            print(f"ğŸ“ å½“å‰é…ç½®: MODEL_PROVIDER={MODEL_PROVIDER}")
            if MODEL_PROVIDER == "openai":
                print(f"ğŸ’¡ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            elif MODEL_PROVIDER == "deepseek":
                print(f"ğŸ’¡ è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"ğŸ“ å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        llm = None

# åˆå§‹åŒ–æ¨¡å‹
init_model()

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: list

# ç®€å•çš„èŠå¤©èŠ‚ç‚¹
def chat_node(state: AgentState) -> AgentState:
    global llm
    messages = state["messages"]
    last_message = messages[-1]
    
    if isinstance(last_message, HumanMessage):
        user_input = last_message.content
        print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
        
        try:
            if llm is not None:
                print(f"ğŸ¤– ä½¿ç”¨ {MODEL_PROVIDER} æ¨¡å‹å¤„ç†è¯·æ±‚...")
                
                # å‡†å¤‡å¯¹è¯å†å²
                chat_messages = []
                
                # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
                from langchain_core.messages import SystemMessage
                system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")
                chat_messages.append(system_msg)
                
                # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆæœ€å¤šä¿ç•™æœ€è¿‘10æ¡ï¼‰
                recent_messages = messages[-10:] if len(messages) > 10 else messages
                for msg in recent_messages:
                    chat_messages.append(msg)
                
                # è°ƒç”¨å¤§æ¨¡å‹
                if MODEL_PROVIDER == "openai" or MODEL_PROVIDER == "deepseek":
                    response = llm.invoke(chat_messages)
                    ai_response = response.content
                elif MODEL_PROVIDER == "ollama":
                    # Ollama å¯èƒ½éœ€è¦ä¸åŒçš„è°ƒç”¨æ–¹å¼
                    response = llm.invoke(user_input)
                    ai_response = response if isinstance(response, str) else str(response)
                else:
                    ai_response = "æ¨¡å‹é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥è®¾ç½®ã€‚"
                    
                print(f"âœ… AIå›å¤: {ai_response[:100]}...")
                
            else:
                # æ¨¡æ‹Ÿæ¨¡å¼å›å¤
                print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼å›å¤")
                user_text = str(user_input)  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                if "ä½ å¥½" in user_text or "hello" in user_text.lower():
                    ai_response = "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                elif "å¤©æ°”" in user_text:
                    ai_response = "æŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ— æ³•è·å–å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚ä½ å¯ä»¥æŸ¥çœ‹å¤©æ°”åº”ç”¨æˆ–ç½‘ç«™è·å–å‡†ç¡®çš„å¤©æ°”é¢„æŠ¥ã€‚\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                elif "æ—¶é—´" in user_text:
                    from datetime import datetime
                    current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
                    ai_response = f"å½“å‰æ—¶é—´æ˜¯ï¼š{current_time}\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                else:
                    ai_response = f"æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼š{user_text}ã€‚\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ä»¥è·å¾—æ™ºèƒ½å›å¤ã€‚"
        
        except Exception as e:
            print(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            ai_response = f"æŠ±æ­‰ï¼Œå¤„ç†ä½ çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ã€‚"
        
        response = AIMessage(content=ai_response)
        return {"messages": messages + [response]}
    
    return {"messages": messages}

# åˆ›å»ºå›¾
def create_graph():
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("chat", chat_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("chat")
    
    # æ·»åŠ ç»“æŸ
    workflow.add_edge("chat", END)
    
    # ç¼–è¯‘å›¾
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="LangGraph Test Server")

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],  # å…è®¸å‰ç«¯è®¿é—®
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

graph = create_graph()

@app.get("/")
async def root():
    return {"message": "LangGraph Server is running"}

@app.get("/info")
async def info():
    return {
        "assistant_id": "agent",
        "graph_id": "agent",
        "status": "running"
    }

# æ•°æ®æ¨¡å‹
class ThreadSearchRequest(BaseModel):
    limit: int = 10
    offset: int = 0
    metadata: dict = {}
    status: str = "idle"

class RunRequest(BaseModel):
    input: dict
    stream_mode: list = ["updates"]
    assistant_id: str = "agent"
    thread_id: Optional[str] = None
    config: Optional[dict] = None
    metadata: Optional[dict] = None
    multitask_strategy: Optional[str] = None
    stream_subgraphs: bool = False
    stream_resumable: bool = False
    on_disconnect: str = "continue"

@app.post("/threads/search")
async def search_threads():
    """æœç´¢çº¿ç¨‹"""
    # æ ¹æ®LangGraph APIè§„èŒƒï¼Œç›´æ¥è¿”å›threadsæ•°ç»„
    return []

@app.post("/threads/{thread_id}/runs/stream")
async def stream_run(thread_id: str, request: Request):
    """æµå¼è¿è¡Œå¯¹è¯"""
    global llm
    
    # ç›´æ¥ä»åŸå§‹è¯·æ±‚ä¸­è§£æJSON
    try:
        body = await request.json()
        print(f"ğŸ” å®Œæ•´è¯·æ±‚ä½“: {json.dumps(body, indent=2, ensure_ascii=False)}")

        # æå–ç”¨æˆ·æ¶ˆæ¯
        user_message = "ä½ å¥½"
        if body and "input" in body and "messages" in body["input"]:
            messages = body["input"]["messages"]
            if messages and len(messages) > 0:
                last_message = messages[-1]
                if "content" in last_message:
                    if isinstance(last_message["content"], list) and len(last_message["content"]) > 0:
                        # å¤„ç†æ•°ç»„æ ¼å¼çš„content
                        content_item = last_message["content"][0]
                        if isinstance(content_item, dict) and "text" in content_item:
                            user_message = content_item["text"]
                        else:
                            user_message = str(content_item)
                    elif isinstance(last_message["content"], str):
                        user_message = last_message["content"]

        print(f"ğŸ’¬ ç”¨æˆ·æ¶ˆæ¯: {user_message}")

        # æå–stream_mode
        stream_mode = body.get("stream_mode", ["updates"])
        print(f"ğŸ“¡ Stream Mode: {stream_mode}")

    except Exception as e:
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¶ˆæ¯
        print(f"âŒ è¯·æ±‚è§£æå¤±è´¥: {e}")
        user_message = "ä½ å¥½"
        stream_mode = ["updates"]
    
    async def generate_stream():
        # æµå¼å“åº”
        run_id = str(uuid.uuid4())
        
        # ç”ŸæˆAIå›å¤
        try:
            if llm is not None:
                print(f"ğŸ¤– ä½¿ç”¨ {MODEL_PROVIDER} æ¨¡å‹å¤„ç†æµå¼è¯·æ±‚...")
                
                # å‡†å¤‡æ¶ˆæ¯
                if MODEL_PROVIDER == "openai" or MODEL_PROVIDER == "deepseek":
                    from langchain_core.messages import SystemMessage, HumanMessage
                    chat_messages = [
                        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"),
                        HumanMessage(content=user_message)
                    ]

                    # ä½¿ç”¨æµå¼è°ƒç”¨è€Œä¸æ˜¯æ™®é€šè°ƒç”¨
                    ai_response = ""
                    print("ğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆå›å¤...")
                    for chunk in llm.stream(chat_messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            content = str(chunk.content) if chunk.content else ""
                            ai_response += content
                            print(f"ğŸ“ æ”¶åˆ°chunk: {content}")

                elif MODEL_PROVIDER == "ollama":
                    # Ollamaä¹Ÿæ”¯æŒæµå¼
                    ai_response = ""
                    for chunk in llm.stream(user_message):
                        if isinstance(chunk, str):
                            ai_response += chunk
                        elif hasattr(chunk, 'content'):
                            content = str(chunk.content) if chunk.content else ""
                            ai_response += content
                else:
                    ai_response = "æ¨¡å‹é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥è®¾ç½®ã€‚"

                print(f"âœ… AIæµå¼å›å¤å®Œæˆ: {ai_response[:100]}...")
                
            else:
                # æ¨¡æ‹Ÿæ¨¡å¼å›å¤
                print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼æµå¼å›å¤")
                user_text = str(user_message)
                if "ä½ å¥½" in user_text or "hello" in user_text.lower():
                    ai_response = "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                else:
                    ai_response = f"æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼š{user_text}ã€‚\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ä»¥è·å¾—æ™ºèƒ½å›å¤ã€‚"
        
        except Exception as e:
            print(f"âŒ æµå¼æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            ai_response = f"æŠ±æ­‰ï¼Œå¤„ç†ä½ çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ã€‚"
        
        # è¿”å›ç¬¦åˆLangGraph SDKæœŸæœ›çš„JSON Linesæ ¼å¼
        # 1. å‘é€è¿è¡Œå¼€å§‹
        yield json.dumps({"run_id": run_id, "thread_id": thread_id}) + "\n"

        await asyncio.sleep(0.1)

        # 2. æ ¹æ®stream_modeå‘é€ç›¸åº”æ ¼å¼çš„æ¶ˆæ¯æ›´æ–°
        response_text = str(ai_response)
        message_id = str(uuid.uuid4())
        user_message_id = str(uuid.uuid4())

        # ä¸ºæ¯ä¸ªstream_modeç”Ÿæˆç›¸åº”çš„æ•°æ®
        for mode in stream_mode:
            if mode == "values":
                # valuesæ¨¡å¼ï¼šå‘é€å®Œæ•´çŠ¶æ€ï¼ŒåŒ…æ‹¬ç”¨æˆ·æ¶ˆæ¯å’ŒAIå›å¤
                state_data = {
                    "messages": [
                        {
                            "id": user_message_id,
                            "type": "human",
                            "content": [
                                {
                                    "type": "text",
                                    "text": str(user_message)
                                }
                            ]
                        },
                        {
                            "id": message_id,
                            "type": "ai",
                            "content": [
                                {
                                    "type": "text",
                                    "text": response_text
                                }
                            ]
                        }
                    ]
                }
                yield json.dumps(state_data) + "\n"
            elif mode == "updates":
                # updatesæ¨¡å¼ï¼šå‘é€èŠ‚ç‚¹æ›´æ–°
                update_data = {
                    "chat": {  # ä½¿ç”¨èŠ‚ç‚¹å
                        "messages": [
                            {
                                "id": message_id,
                                "type": "ai",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": response_text
                                    }
                                ]
                            }
                        ]
                    }
                }
                yield json.dumps(update_data) + "\n"
            elif mode == "messages-tuple":
                # messages-tupleæ¨¡å¼ï¼šå‘é€LLM tokenä¿¡æ¯
                message_chunk = {
                    "type": "ai",
                    "content": response_text
                }
                metadata = {
                    "langgraph_node": "chat",
                    "run_id": run_id
                }
                yield json.dumps([message_chunk, metadata]) + "\n"
            elif mode == "custom":
                # customæ¨¡å¼ï¼šå‘é€è‡ªå®šä¹‰æ•°æ®
                custom_data = {
                    "type": "custom",
                    "data": {
                        "message": response_text,
                        "node": "chat"
                    }
                }
                yield json.dumps(custom_data) + "\n"

        await asyncio.sleep(0.1)

        # 3. å‘é€ç»“æŸæ ‡è®°
        yield json.dumps({"status": "complete", "run_id": run_id}) + "\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.post("/threads/{thread_id}/history")
async def get_thread_history(thread_id: str, request: Request):
    """è·å–çº¿ç¨‹å†å²è®°å½•"""
    try:
        body = await request.json()
    except:
        body = {}
    
    # è¿”å›LangGraph SDKæœŸæœ›çš„æ•°ç»„æ ¼å¼
    # SDKæœŸæœ›çš„æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œä¸æ˜¯å¸¦æœ‰valuesçš„å¯¹è±¡
    return []

@app.post("/threads/{thread_id}/messages")
async def add_message_to_thread(thread_id: str, request: Request):
    """å‘çº¿ç¨‹æ·»åŠ æ¶ˆæ¯"""
    try:
        body = await request.json()
        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        if not body or "content" not in body:
            return {"status": "error", "message": "Missing message content"}

        # è¿”å›æˆåŠŸå“åº” - å…¼å®¹LangGraph SDKæ ¼å¼
        return {
            "status": "success",
            "message_id": str(uuid.uuid4()),
            "thread_id": thread_id,
            "created_at": datetime.now().isoformat(),
            "content": body.get("content", ""),
            "type": body.get("type", "human")
        }
    except Exception as e:
        return {"status": "error", "message": f"Failed to add message: {str(e)}"}

@app.post("/threads")
async def create_thread():
    """åˆ›å»ºæ–°çº¿ç¨‹"""
    thread_id = str(uuid.uuid4())
    return {
        "thread_id": thread_id,
        "created_at": datetime.now().isoformat(),
        "metadata": {}
    }

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ LangGraph æµ‹è¯•æœåŠ¡å™¨...")
    print("ğŸ“ æœåŠ¡å™¨åœ°å€: http://localhost:2024")
    print("ğŸ”— Agent Chat UI å¯ä»¥è¿æ¥åˆ°æ­¤æœåŠ¡å™¨")
    
    uvicorn.run(
        "langgraph_server:app",
        host="localhost",
        port=2024,
        reload=True
    )