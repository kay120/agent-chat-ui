#!/usr/bin/env python3
"""
LangGraph æœåŠ¡å™¨ - é›†æˆçœŸå®AIæ¨¡å‹
æ”¯æŒOpenAIã€Ollamaç­‰å¤šç§æ¨¡å‹æä¾›å•†
"""

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, List, Optional
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from langchain_core.messages import HumanMessage, AIMessage
from pydantic import BaseModel
import uuid
from datetime import datetime
import json
import asyncio
import os
from dotenv import load_dotenv
import sqlite3
from contextlib import contextmanager

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ¨¡å‹é…ç½®
MODEL_PROVIDER = os.getenv("MODEL_PROVIDER", "deepseek")  # openai, deepseek, ollama, mock
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama2")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# åˆå§‹åŒ–æ¨¡å‹
llm = None

def init_model():
    global llm
    try:
        if MODEL_PROVIDER == "openai" and OPENAI_API_KEY:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(
                model=OPENAI_MODEL,
                temperature=0.7,
                streaming=True
            )
            # è®¾ç½® API key ä¸ºç¯å¢ƒå˜é‡
            os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
            print(f"âœ… å·²åˆå§‹åŒ– OpenAI æ¨¡å‹: {OPENAI_MODEL}")
        
        elif MODEL_PROVIDER == "deepseek" and DEEPSEEK_API_KEY:
            from langchain_openai import ChatOpenAI
            # è®¾ç½® DeepSeek API é…ç½®
            os.environ["OPENAI_API_KEY"] = DEEPSEEK_API_KEY
            os.environ["OPENAI_API_BASE"] = DEEPSEEK_BASE_URL
            llm = ChatOpenAI(
                model=DEEPSEEK_MODEL,
                temperature=0.7,
                streaming=True,
                base_url=DEEPSEEK_BASE_URL
            )
            print(f"âœ… å·²åˆå§‹åŒ– DeepSeek æ¨¡å‹: {DEEPSEEK_MODEL}")
        
        elif MODEL_PROVIDER == "ollama":
            try:
                from langchain_community.llms import Ollama
                from langchain_core.language_models.chat_models import BaseChatModel
                # ä½¿ç”¨ Ollama ä½œä¸ºåŸºç¡€æ¨¡å‹
                llm = Ollama(
                    model=OLLAMA_MODEL,
                    base_url=OLLAMA_BASE_URL,
                    temperature=0.7
                )
                print(f"âœ… å·²åˆå§‹åŒ– Ollama æ¨¡å‹: {OLLAMA_MODEL}")
            except ImportError:
                print(f"âš ï¸  Ollama å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                llm = None
        
        else:
            print(f"âš ï¸  æœªé…ç½®æœ‰æ•ˆçš„æ¨¡å‹æä¾›å•†ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            print(f"ğŸ“ å½“å‰é…ç½®: MODEL_PROVIDER={MODEL_PROVIDER}")
            if MODEL_PROVIDER == "openai":
                print(f"ğŸ’¡ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            elif MODEL_PROVIDER == "deepseek":
                print(f"ğŸ’¡ è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"ğŸ“ å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        llm = None

# åˆå§‹åŒ–æ¨¡å‹
init_model()

# SQLite æ•°æ®åº“é…ç½®
DB_PATH = os.getenv("SQLITE_DB_PATH", "chat_history.db")

# åˆå§‹åŒ–æ•°æ®åº“
def init_db():
    """åˆå§‹åŒ– SQLite æ•°æ®åº“"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # åˆ›å»ºçº¿ç¨‹è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS threads (
            thread_id TEXT PRIMARY KEY,
            created_at TEXT NOT NULL,
            updated_at TEXT NOT NULL
        )
    """)

    # åˆ›å»ºæ¶ˆæ¯è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS messages (
            id TEXT PRIMARY KEY,
            thread_id TEXT NOT NULL,
            type TEXT NOT NULL,
            content TEXT NOT NULL,
            created_at TEXT NOT NULL,
            FOREIGN KEY (thread_id) REFERENCES threads(thread_id)
        )
    """)

    conn.commit()
    conn.close()
    print(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {DB_PATH}")

# åˆå§‹åŒ–æ•°æ®åº“
init_db()

# æ•°æ®åº“æ“ä½œå‡½æ•°
def save_thread_to_db(thread_id: str, created_at: str = None):
    """ä¿å­˜çº¿ç¨‹åˆ°æ•°æ®åº“"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    if created_at is None:
        created_at = datetime.now().isoformat()

    cursor.execute("""
        INSERT OR IGNORE INTO threads (thread_id, created_at, updated_at)
        VALUES (?, ?, ?)
    """, (thread_id, created_at, created_at))

    conn.commit()
    conn.close()

def save_message_to_db(thread_id: str, msg_id: str, msg_type: str, content: str):
    """ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    created_at = datetime.now().isoformat()

    cursor.execute("""
        INSERT INTO messages (id, thread_id, type, content, created_at)
        VALUES (?, ?, ?, ?, ?)
    """, (msg_id, thread_id, msg_type, content, created_at))

    # æ›´æ–°çº¿ç¨‹çš„ updated_at
    cursor.execute("""
        UPDATE threads SET updated_at = ? WHERE thread_id = ?
    """, (created_at, thread_id))

    conn.commit()
    conn.close()

def load_thread_from_db(thread_id: str):
    """ä»æ•°æ®åº“åŠ è½½çº¿ç¨‹"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # è·å–çº¿ç¨‹ä¿¡æ¯
    cursor.execute("""
        SELECT created_at, updated_at FROM threads WHERE thread_id = ?
    """, (thread_id,))

    thread_row = cursor.fetchone()
    if not thread_row:
        conn.close()
        return None

    created_at, updated_at = thread_row

    # è·å–æ¶ˆæ¯
    cursor.execute("""
        SELECT id, type, content FROM messages
        WHERE thread_id = ?
        ORDER BY created_at ASC
    """, (thread_id,))

    messages = []
    for row in cursor.fetchall():
        msg_id, msg_type, content = row
        messages.append({
            "id": msg_id,
            "type": msg_type,
            "content": content
        })

    conn.close()

    return {
        "messages": messages,
        "created_at": created_at,
        "updated_at": updated_at
    }

def load_all_threads_from_db():
    """ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰çº¿ç¨‹"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT thread_id, created_at, updated_at FROM threads
        ORDER BY updated_at DESC
    """)

    threads = []
    for row in cursor.fetchall():
        thread_id, created_at, updated_at = row

        # è·å–è¯¥çº¿ç¨‹çš„æ¶ˆæ¯
        cursor.execute("""
            SELECT id, type, content FROM messages
            WHERE thread_id = ?
            ORDER BY created_at ASC
        """, (thread_id,))

        messages = []
        for msg_row in cursor.fetchall():
            msg_id, msg_type, content = msg_row
            messages.append({
                "id": msg_id,
                "type": msg_type,
                "content": content
            })

        threads.append({
            "thread_id": thread_id,
            "created_at": created_at,
            "updated_at": updated_at,
            "messages": messages
        })

    conn.close()
    return threads

# çº¿ç¨‹çŠ¶æ€å­˜å‚¨ï¼ˆå†…å­˜ç¼“å­˜ï¼Œç”¨äºå¿«é€Ÿè®¿é—®ï¼‰
thread_states = {}

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: list

# ç®€å•çš„èŠå¤©èŠ‚ç‚¹
def chat_node(state: AgentState) -> AgentState:
    global llm
    messages = state["messages"]
    last_message = messages[-1]
    
    if isinstance(last_message, HumanMessage):
        user_input = last_message.content
        print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
        
        try:
            if llm is not None:
                print(f"ğŸ¤– ä½¿ç”¨ {MODEL_PROVIDER} æ¨¡å‹å¤„ç†è¯·æ±‚...")
                
                # å‡†å¤‡å¯¹è¯å†å²
                chat_messages = []
                
                # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
                from langchain_core.messages import SystemMessage
                system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")
                chat_messages.append(system_msg)
                
                # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆæœ€å¤šä¿ç•™æœ€è¿‘10æ¡ï¼‰
                recent_messages = messages[-10:] if len(messages) > 10 else messages
                for msg in recent_messages:
                    chat_messages.append(msg)
                
                # è°ƒç”¨å¤§æ¨¡å‹
                if MODEL_PROVIDER == "openai" or MODEL_PROVIDER == "deepseek":
                    response = llm.invoke(chat_messages)
                    ai_response = response.content
                elif MODEL_PROVIDER == "ollama":
                    # Ollama å¯èƒ½éœ€è¦ä¸åŒçš„è°ƒç”¨æ–¹å¼
                    response = llm.invoke(user_input)
                    ai_response = response if isinstance(response, str) else str(response)
                else:
                    ai_response = "æ¨¡å‹é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥è®¾ç½®ã€‚"
                    
                print(f"âœ… AIå›å¤: {ai_response[:100]}...")
                
            else:
                # æ¨¡æ‹Ÿæ¨¡å¼å›å¤
                print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼å›å¤")
                user_text = str(user_input)  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                if "ä½ å¥½" in user_text or "hello" in user_text.lower():
                    ai_response = "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                elif "å¤©æ°”" in user_text:
                    ai_response = "æŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ— æ³•è·å–å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚ä½ å¯ä»¥æŸ¥çœ‹å¤©æ°”åº”ç”¨æˆ–ç½‘ç«™è·å–å‡†ç¡®çš„å¤©æ°”é¢„æŠ¥ã€‚\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                elif "æ—¶é—´" in user_text:
                    from datetime import datetime
                    current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
                    ai_response = f"å½“å‰æ—¶é—´æ˜¯ï¼š{current_time}\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                else:
                    ai_response = f"æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼š{user_text}ã€‚\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ä»¥è·å¾—æ™ºèƒ½å›å¤ã€‚"
        
        except Exception as e:
            print(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            ai_response = f"æŠ±æ­‰ï¼Œå¤„ç†ä½ çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ã€‚"
        
        response = AIMessage(content=ai_response)
        return {"messages": messages + [response]}
    
    return {"messages": messages}

# åˆ›å»ºå›¾
def create_graph():
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("chat", chat_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("chat")
    
    # æ·»åŠ ç»“æŸ
    workflow.add_edge("chat", END)
    
    # ç¼–è¯‘å›¾
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="LangGraph Test Server")

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],  # å…è®¸å‰ç«¯è®¿é—®
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

graph = create_graph()

@app.get("/")
async def root():
    return {"message": "LangGraph Server is running"}

@app.get("/info")
async def info():
    return {
        "assistant_id": "agent",
        "graph_id": "agent",
        "status": "running"
    }

# æ•°æ®æ¨¡å‹
class ThreadSearchRequest(BaseModel):
    limit: int = 10
    offset: int = 0
    metadata: dict = {}
    status: str = "idle"

class RunRequest(BaseModel):
    input: dict
    stream_mode: list = ["updates"]
    assistant_id: str = "agent"
    thread_id: Optional[str] = None
    config: Optional[dict] = None
    metadata: Optional[dict] = None
    multitask_strategy: Optional[str] = None
    stream_subgraphs: bool = False
    stream_resumable: bool = False
    on_disconnect: str = "continue"

@app.post("/threads/search")
async def search_threads():
    """æœç´¢çº¿ç¨‹"""
    # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰çº¿ç¨‹
    db_threads = load_all_threads_from_db()

    threads = []
    for thread_data in db_threads:
        threads.append({
            "thread_id": thread_data["thread_id"],
            "created_at": thread_data["created_at"],
            "updated_at": thread_data["updated_at"],
            "metadata": {},
            "values": {
                "messages": thread_data["messages"]
            }
        })

    print(f"ğŸ“‹ æœç´¢çº¿ç¨‹: æ‰¾åˆ° {len(threads)} ä¸ªçº¿ç¨‹")
    return threads

@app.post("/threads/{thread_id}/runs/stream")
async def stream_run(thread_id: str, request: Request):
    """æµå¼è¿è¡Œå¯¹è¯"""
    global llm
    
    # åˆå§‹åŒ–çº¿ç¨‹çŠ¶æ€ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if thread_id not in thread_states:
        # å…ˆå°è¯•ä»æ•°æ®åº“åŠ è½½
        db_thread = load_thread_from_db(thread_id)
        if db_thread:
            thread_states[thread_id] = db_thread
            print(f"ğŸ“¥ ä»æ•°æ®åº“åŠ è½½çº¿ç¨‹: {thread_id}")
        else:
            # åˆ›å»ºæ–°çº¿ç¨‹
            created_at = datetime.now().isoformat()
            thread_states[thread_id] = {
                "messages": [],
                "created_at": created_at,
                "updated_at": created_at
            }
            # ä¿å­˜åˆ°æ•°æ®åº“
            save_thread_to_db(thread_id, created_at)
            print(f"ğŸ†• åˆ›å»ºæ–°çº¿ç¨‹å¹¶ä¿å­˜åˆ°æ•°æ®åº“: {thread_id}")

    # ç›´æ¥ä»åŸå§‹è¯·æ±‚ä¸­è§£æJSON
    try:
        body = await request.json()
        print(f"ğŸ” å®Œæ•´è¯·æ±‚ä½“: {json.dumps(body, indent=2, ensure_ascii=False)}")

        # æå–ç”¨æˆ·æ¶ˆæ¯
        user_message = "ä½ å¥½"
        if body and "input" in body and "messages" in body["input"]:
            messages = body["input"]["messages"]
            if messages and len(messages) > 0:
                last_message = messages[-1]
                if "content" in last_message:
                    if isinstance(last_message["content"], list) and len(last_message["content"]) > 0:
                        # å¤„ç†æ•°ç»„æ ¼å¼çš„content
                        content_item = last_message["content"][0]
                        if isinstance(content_item, dict) and "text" in content_item:
                            user_message = content_item["text"]
                        else:
                            user_message = str(content_item)
                    elif isinstance(last_message["content"], str):
                        user_message = last_message["content"]

        print(f"ğŸ’¬ ç”¨æˆ·æ¶ˆæ¯: {user_message}")

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°çº¿ç¨‹çŠ¶æ€
        user_msg_id = str(uuid.uuid4())
        user_msg_obj = {
            "id": user_msg_id,
            "type": "human",
            "content": user_message
        }
        thread_states[thread_id]["messages"].append(user_msg_obj)
        thread_states[thread_id]["updated_at"] = datetime.now().isoformat()

        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ•°æ®åº“
        save_message_to_db(thread_id, user_msg_id, "human", user_message)
        print(f"ğŸ’¾ ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")

        # æå–stream_mode
        stream_mode = body.get("stream_mode", ["updates"])
        print(f"ğŸ“¡ Stream Mode: {stream_mode}")

    except Exception as e:
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¶ˆæ¯
        print(f"âŒ è¯·æ±‚è§£æå¤±è´¥: {e}")
        user_message = "ä½ å¥½"
        stream_mode = ["updates"]
    
    async def generate_stream():
        # æµå¼å“åº”
        run_id = str(uuid.uuid4())
        
        # ç”ŸæˆAIå›å¤
        try:
            if llm is not None:
                print(f"ğŸ¤– ä½¿ç”¨ {MODEL_PROVIDER} æ¨¡å‹å¤„ç†æµå¼è¯·æ±‚...")
                
                # å‡†å¤‡æ¶ˆæ¯
                if MODEL_PROVIDER == "openai" or MODEL_PROVIDER == "deepseek":
                    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

                    # æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²
                    chat_messages = [
                        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")
                    ]

                    # æ·»åŠ å†å²æ¶ˆæ¯
                    for msg in thread_states[thread_id]["messages"]:
                        if msg["type"] == "human":
                            chat_messages.append(HumanMessage(content=msg["content"]))
                        elif msg["type"] == "ai":
                            chat_messages.append(AIMessage(content=msg["content"]))

                    print(f"ğŸ“š å¯¹è¯å†å²é•¿åº¦: {len(chat_messages)} æ¡æ¶ˆæ¯")

                    # ğŸ”„ çœŸæ­£çš„æµå¼è¾“å‡ºï¼šè¾¹ç”Ÿæˆè¾¹å‘é€
                    ai_response = ""
                    print("ğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆå›å¤...")

                    # å…ˆå‘é€ metadata äº‹ä»¶
                    yield f"event: metadata\n"
                    yield f"data: {json.dumps({'run_id': run_id, 'thread_id': thread_id})}\n\n"

                    message_id = str(uuid.uuid4())

                    for chunk in llm.stream(chat_messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            content = str(chunk.content) if chunk.content else ""
                            ai_response += content
                            print(f"ğŸ“ æ”¶åˆ°chunk: {content}")

                            # ğŸš€ ç«‹å³å‘é€æµå¼æ•°æ®
                            if "messages" in stream_mode:
                                message_data = [{
                                    "id": message_id,
                                    "type": "ai",
                                    "content": ai_response  # å‘é€ç´¯ç§¯çš„å†…å®¹
                                }]
                                yield f"event: messages/partial\n"
                                yield f"data: {json.dumps(message_data)}\n\n"

                            await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ

                    print(f"âœ… AIæµå¼å›å¤å®Œæˆ: {ai_response[:100]}...")

                elif MODEL_PROVIDER == "ollama":
                    # Ollamaä¹Ÿæ”¯æŒæµå¼
                    ai_response = ""
                    message_id = str(uuid.uuid4())

                    # å…ˆå‘é€ metadata
                    yield f"event: metadata\n"
                    yield f"data: {json.dumps({'run_id': run_id, 'thread_id': thread_id})}\n\n"

                    for chunk in llm.stream(user_message):
                        if isinstance(chunk, str):
                            ai_response += chunk
                        elif hasattr(chunk, 'content'):
                            content = str(chunk.content) if chunk.content else ""
                            ai_response += content

                        # ç«‹å³å‘é€æµå¼æ•°æ®
                        if "messages" in stream_mode:
                            message_data = [{
                                "id": message_id,
                                "type": "ai",
                                "content": ai_response
                            }]
                            yield f"event: messages/partial\n"
                            yield f"data: {json.dumps(message_data)}\n\n"

                        await asyncio.sleep(0)

                    print(f"âœ… AIæµå¼å›å¤å®Œæˆ: {ai_response[:100]}...")
                else:
                    ai_response = "æ¨¡å‹é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥è®¾ç½®ã€‚"

            else:
                # æ¨¡æ‹Ÿæ¨¡å¼å›å¤
                print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼æµå¼å›å¤")
                user_text = str(user_message)
                if "ä½ å¥½" in user_text or "hello" in user_text.lower():
                    ai_response = "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ã€‚"
                else:
                    ai_response = f"æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼š{user_text}ã€‚\n\nâš ï¸ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·é…ç½®çœŸå®çš„AIæ¨¡å‹ä»¥è·å¾—æ™ºèƒ½å›å¤ã€‚"

        except Exception as e:
            print(f"âŒ æµå¼æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            ai_response = f"æŠ±æ­‰ï¼Œå¤„ç†ä½ çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ã€‚"

        # ä¿å­˜ AI å›å¤åˆ°çº¿ç¨‹çŠ¶æ€
        response_text = str(ai_response)
        ai_msg_id = str(uuid.uuid4())
        ai_msg_obj = {
            "id": ai_msg_id,
            "type": "ai",
            "content": response_text
        }
        thread_states[thread_id]["messages"].append(ai_msg_obj)
        thread_states[thread_id]["updated_at"] = datetime.now().isoformat()

        # ä¿å­˜ AI å›å¤åˆ°æ•°æ®åº“
        save_message_to_db(thread_id, ai_msg_id, "ai", response_text)
        print(f"ğŸ’¾ å·²ä¿å­˜åˆ°çº¿ç¨‹çŠ¶æ€å’Œæ•°æ®åº“ï¼Œå½“å‰æ¶ˆæ¯æ•°: {len(thread_states[thread_id]['messages'])}")

        # å‘é€æœ€ç»ˆçš„ values äº‹ä»¶ï¼ˆåŒ…å«å®Œæ•´å¯¹è¯ï¼‰
        message_id = str(uuid.uuid4())
        user_message_id = str(uuid.uuid4())

        # å‘é€ values äº‹ä»¶
        if "values" in stream_mode:
            state_data = {
                "messages": [
                    {
                        "id": user_message_id,
                        "type": "human",
                        "content": str(user_message)
                    },
                    {
                        "id": message_id,
                        "type": "ai",
                        "content": response_text
                    }
                ]
            }
            yield f"event: values\n"
            yield f"data: {json.dumps(state_data)}\n\n"

        await asyncio.sleep(0.1)

        # å‘é€ç»“æŸæ ‡è®°
        yield f"event: end\n"
        yield f"data: {json.dumps({'status': 'complete', 'run_id': run_id})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",  # SSE æ ¼å¼
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ nginx ç¼“å†²
        }
    )

@app.post("/threads/{thread_id}/history")
async def get_thread_history(thread_id: str, request: Request):
    """è·å–çº¿ç¨‹å†å²è®°å½•"""
    try:
        body = await request.json()
    except:
        body = {}

    # è¿”å›çº¿ç¨‹çš„æ¶ˆæ¯å†å²
    if thread_id in thread_states:
        messages = thread_states[thread_id]["messages"]
        print(f"ğŸ“œ è·å–çº¿ç¨‹å†å²: {thread_id}, æ¶ˆæ¯æ•°: {len(messages)}")
        return messages
    else:
        print(f"âš ï¸  çº¿ç¨‹ä¸å­˜åœ¨: {thread_id}")
        return []

@app.post("/threads/{thread_id}/messages")
async def add_message_to_thread(thread_id: str, request: Request):
    """å‘çº¿ç¨‹æ·»åŠ æ¶ˆæ¯"""
    try:
        body = await request.json()
        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        if not body or "content" not in body:
            return {"status": "error", "message": "Missing message content"}

        # è¿”å›æˆåŠŸå“åº” - å…¼å®¹LangGraph SDKæ ¼å¼
        return {
            "status": "success",
            "message_id": str(uuid.uuid4()),
            "thread_id": thread_id,
            "created_at": datetime.now().isoformat(),
            "content": body.get("content", ""),
            "type": body.get("type", "human")
        }
    except Exception as e:
        return {"status": "error", "message": f"Failed to add message: {str(e)}"}

@app.post("/threads")
async def create_thread():
    """åˆ›å»ºæ–°çº¿ç¨‹"""
    thread_id = str(uuid.uuid4())
    created_at = datetime.now().isoformat()

    # åˆå§‹åŒ–çº¿ç¨‹çŠ¶æ€
    thread_states[thread_id] = {
        "messages": [],
        "created_at": created_at,
        "updated_at": created_at
    }

    # ä¿å­˜åˆ°æ•°æ®åº“
    save_thread_to_db(thread_id, created_at)
    print(f"ğŸ†• åˆ›å»ºæ–°çº¿ç¨‹: {thread_id}")

    return {
        "thread_id": thread_id,
        "created_at": created_at,
        "metadata": {}
    }

@app.get("/threads/{thread_id}/state")
async def get_thread_state(thread_id: str):
    """è·å–çº¿ç¨‹çŠ¶æ€"""
    if thread_id not in thread_states:
        raise HTTPException(status_code=404, detail="Thread not found")

    state = thread_states[thread_id]
    return {
        "values": {
            "messages": state["messages"]
        },
        "next": [],
        "config": {
            "configurable": {
                "thread_id": thread_id
            }
        }
    }

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ LangGraph æµ‹è¯•æœåŠ¡å™¨...")
    print("ğŸ“ æœåŠ¡å™¨åœ°å€: http://localhost:2024")
    print("ğŸ”— Agent Chat UI å¯ä»¥è¿æ¥åˆ°æ­¤æœåŠ¡å™¨")
    
    uvicorn.run(
        "langgraph_server:app",
        host="localhost",
        port=2024,
        reload=True
    )