"""
LangGraph æœåŠ¡æ¨¡å—
"""
import uuid
import json
from typing import AsyncGenerator, Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from ..models.state import State
from .llm_service import llm_service
from .thread_service import thread_service


class GraphService:
    """LangGraph æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– Graph æœåŠ¡"""
        self.graph = self._create_graph()
        print("âœ… Graph æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _create_graph(self):
        """åˆ›å»º LangGraph"""
        # åˆ›å»ºå›¾
        workflow = StateGraph(State)
        
        # æ·»åŠ èŠå¤©èŠ‚ç‚¹
        workflow.add_node("chatbot", self._chatbot_node)
        
        # è®¾ç½®å…¥å£å’Œå‡ºå£
        workflow.add_edge(START, "chatbot")
        workflow.add_edge("chatbot", END)
        
        # ç¼–è¯‘å›¾ï¼ˆä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹ï¼‰
        memory = MemorySaver()
        graph = workflow.compile(checkpointer=memory)
        
        return graph
    
    def _chatbot_node(self, state: State, config: RunnableConfig) -> Dict[str, Any]:
        """
        èŠå¤©æœºå™¨äººèŠ‚ç‚¹
        
        Args:
            state: å½“å‰çŠ¶æ€
            config: è¿è¡Œé…ç½®
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        messages = state["messages"]
        print(f"ğŸ¤– Chatbot node called with {len(messages)} messages")
        
        # è°ƒç”¨ LLM
        llm = llm_service.get_llm()
        response = llm.invoke(messages)
        
        return {"messages": [response]}
    
    async def stream_response(
        self,
        input_messages: list,
        thread_id: str,
        stream_mode: list = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼å¤„ç†å“åº”

        Args:
            input_messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            thread_id: çº¿ç¨‹ID
            stream_mode: æµå¼æ¨¡å¼åˆ—è¡¨

        Yields:
            SSE æ ¼å¼çš„æ•°æ®æµ
        """
        if stream_mode is None:
            stream_mode = ["messages", "values"]

        # ç”Ÿæˆ run_id
        run_id = str(uuid.uuid4())
        print(f"ğŸš€ å¼€å§‹æµå¼å¤„ç†ï¼Œçº¿ç¨‹ID: {thread_id}, Run ID: {run_id}")

        # å‘é€å…ƒæ•°æ®äº‹ä»¶
        yield f"event: metadata\n"
        yield f"data: {json.dumps({'run_id': run_id, 'thread_id': thread_id})}\n\n"
        
        # åŠ è½½çº¿ç¨‹å†å²
        thread = thread_service.get_thread(thread_id)
        if not thread:
            # çº¿ç¨‹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çº¿ç¨‹
            thread_service.create_thread(thread_id)
            thread = thread_service.get_thread(thread_id)

        # æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²
        messages = []
        for msg in thread["messages"]:
            if msg["type"] == "human":
                messages.append(HumanMessage(content=msg["content"], id=msg.get("id")))
            elif msg["type"] == "ai":
                messages.append(AIMessage(content=msg["content"], id=msg.get("id")))

        # æ·»åŠ æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        user_message = None
        for msg in input_messages:
            if msg.get("role") == "user":
                content = msg["content"]
                if isinstance(content, list):
                    # å¤„ç†å¤æ‚å†…å®¹
                    text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
                    content = " ".join(text_parts)
                user_message = content
                user_msg_id = str(uuid.uuid4())
                messages.append(HumanMessage(content=content, id=user_msg_id))

                # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ•°æ®åº“
                thread_service.save_message(thread_id, user_msg_id, "human", content)
                break

        print(f"ğŸ“š å¯¹è¯å†å²é•¿åº¦: {len(messages)} æ¡æ¶ˆæ¯")
        
        # é…ç½®
        config = {
            "configurable": {
                "thread_id": thread_id,
                "run_id": run_id,
            }
        }
        
        # æµå¼å¤„ç†
        try:
            chunk_count = 0
            ai_response_content = ""
            ai_msg_id = str(uuid.uuid4())

            # ä½¿ç”¨ LLM ç›´æ¥æµå¼ç”Ÿæˆï¼ˆä¸ä½¿ç”¨ graphï¼‰
            llm = llm_service.get_llm()

            print(f"ğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆå›å¤...")
            async for chunk in llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    content = str(chunk.content) if chunk.content else ""
                    ai_response_content += content
                    chunk_count += 1

                    # å‘é€æµå¼æ¶ˆæ¯äº‹ä»¶
                    if "messages" in stream_mode:
                        message_data = [{
                            "id": ai_msg_id,
                            "type": "ai",
                            "content": ai_response_content
                        }]
                        yield f"event: messages/partial\n"
                        yield f"data: {json.dumps(message_data)}\n\n"

                    print(f"ğŸ“ æ”¶åˆ°chunk: {content}")

            print(f"âœ… AIæµå¼å›å¤å®Œæˆ: {ai_response_content[:100]}...")

            # ä¿å­˜ AI å›å¤åˆ°æ•°æ®åº“
            thread_service.save_message(thread_id, ai_msg_id, "ai", ai_response_content)

            # å‘é€æœ€ç»ˆçš„ values äº‹ä»¶
            if "values" in stream_mode:
                final_messages = thread["messages"] + [
                    {"id": user_msg_id, "type": "human", "content": user_message},
                    {"id": ai_msg_id, "type": "ai", "content": ai_response_content}
                ]
                yield f"event: values\n"
                yield f"data: {json.dumps({'messages': final_messages})}\n\n"

            # å‘é€ç»“æŸäº‹ä»¶
            yield f"event: end\n"
            yield f"data: {json.dumps({})}\n\n"

        except Exception as e:
            print(f"âŒ æµå¼å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            yield f"event: error\n"
            yield f"data: {json.dumps({'error': str(e)})}\n\n"


# å…¨å±€ Graph æœåŠ¡å®ä¾‹
graph_service = GraphService()

