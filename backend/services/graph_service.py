"""
LangGraph æœåŠ¡æ¨¡å—
"""
import uuid
import json
from typing import AsyncGenerator, Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from ..models.state import State
from .llm_service import llm_service
from .thread_service import thread_service


class GraphService:
    """LangGraph æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– Graph æœåŠ¡"""
        self.graph = self._create_graph()
        print("âœ… Graph æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _create_graph(self):
        """åˆ›å»º LangGraph"""
        # åˆ›å»ºå›¾
        workflow = StateGraph(State)
        
        # æ·»åŠ èŠå¤©èŠ‚ç‚¹
        workflow.add_node("chatbot", self._chatbot_node)
        
        # è®¾ç½®å…¥å£å’Œå‡ºå£
        workflow.add_edge(START, "chatbot")
        workflow.add_edge("chatbot", END)
        
        # ç¼–è¯‘å›¾ï¼ˆä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹ï¼‰
        memory = MemorySaver()
        graph = workflow.compile(checkpointer=memory)
        
        return graph
    
    def _chatbot_node(self, state: State, config: RunnableConfig) -> Dict[str, Any]:
        """
        èŠå¤©æœºå™¨äººèŠ‚ç‚¹
        
        Args:
            state: å½“å‰çŠ¶æ€
            config: è¿è¡Œé…ç½®
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        messages = state["messages"]
        print(f"ğŸ¤– Chatbot node called with {len(messages)} messages")
        
        # è°ƒç”¨ LLM
        llm = llm_service.get_llm()
        response = llm.invoke(messages)
        
        return {"messages": [response]}
    
    async def stream_response(
        self,
        input_messages: list,
        thread_id: str | None = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼å¤„ç†å“åº”
        
        Args:
            input_messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            thread_id: çº¿ç¨‹IDï¼ˆå¯é€‰ï¼‰
            
        Yields:
            SSE æ ¼å¼çš„æ•°æ®æµ
        """
        # å¦‚æœæ²¡æœ‰æä¾› thread_idï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„
        if not thread_id:
            thread_id = str(uuid.uuid4())
            print(f"ğŸ†• ç”Ÿæˆæ–°çº¿ç¨‹ID: {thread_id}")
        
        # ç”Ÿæˆ run_id
        run_id = str(uuid.uuid4())
        print(f"ğŸš€ å¼€å§‹æµå¼å¤„ç†ï¼Œçº¿ç¨‹ID: {thread_id}, Run ID: {run_id}")
        
        # å‘é€å“åº”å¤´ä¿¡æ¯
        yield f"X-Thread-ID: {thread_id}\n"
        yield f"X-Run-ID: {run_id}\n\n"
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = []
        for msg in input_messages:
            if msg["type"] == "human":
                content = msg["content"]
                if isinstance(content, list):
                    # å¤„ç†å¤æ‚å†…å®¹
                    text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
                    content = " ".join(text_parts)
                messages.append(HumanMessage(content=content, id=msg.get("id")))
            elif msg["type"] == "ai":
                messages.append(AIMessage(content=msg["content"], id=msg.get("id")))
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
        thread_service.save_thread(thread_id, [
            {
                "id": msg.id,
                "type": msg.type,
                "content": msg.content,
                "timestamp": str(msg.id) if hasattr(msg, 'id') else None
            }
            for msg in messages
        ])
        
        # é…ç½®
        config = {
            "configurable": {
                "thread_id": thread_id,
                "run_id": run_id,
            }
        }
        
        # æµå¼å¤„ç†
        try:
            chunk_count = 0
            ai_response_content = ""
            
            async for event in self.graph.astream_events(
                {"messages": messages},
                config,
                version="v2"
            ):
                kind = event.get("event")
                
                # å¤„ç†æµå¼è¾“å‡º
                if kind == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk")
                    if chunk and hasattr(chunk, "content") and chunk.content:
                        chunk_count += 1
                        ai_response_content += chunk.content

                        # æ„å»ºå½“å‰æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«AIå›å¤ï¼‰
                        current_messages = messages + [AIMessage(content=ai_response_content)]

                        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                        serializable_messages = [
                            {
                                "id": msg.id if hasattr(msg, 'id') else None,
                                "type": msg.type,
                                "content": msg.content,
                            }
                            for msg in current_messages
                        ]

                        # å‘é€ SSE æ•°æ®
                        data = {
                            "event": "values",
                            "data": {
                                "messages": serializable_messages
                            }
                        }
                        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

                        print(f"ğŸ“¦ æ”¶åˆ°chunk #{chunk_count}: {chunk.content[:50]}...")
            
            print(f"âœ… æµå¼å¤„ç†å®Œæˆï¼Œå…± {chunk_count} ä¸ªchunks")
            
            # ä¿å­˜å®Œæ•´å¯¹è¯åˆ°å†å²è®°å½•
            final_messages = messages + [AIMessage(content=ai_response_content)]
            thread_service.save_thread(thread_id, [
                {
                    "id": msg.id if hasattr(msg, 'id') else None,
                    "type": msg.type,
                    "content": msg.content,
                    "timestamp": str(msg.id) if hasattr(msg, 'id') else None
                }
                for msg in final_messages
            ])
            
        except Exception as e:
            print(f"âŒ æµå¼å¤„ç†é”™è¯¯: {e}")
            error_data = {
                "event": "error",
                "data": {"error": str(e)}
            }
            yield f"data: {json.dumps(error_data)}\n\n"


# å…¨å±€ Graph æœåŠ¡å®ä¾‹
graph_service = GraphService()

