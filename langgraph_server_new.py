#!/usr/bin/env python3
"""
æ ‡å‡†LangGraphæœåŠ¡å™¨å®ç°
åŸºäºLangGraphå®˜æ–¹æ–‡æ¡£å’Œæœ€ä½³å®è·µ
"""

import os
import json
import uuid
import asyncio
from datetime import datetime
from typing import Optional, List, Dict, Any, TypedDict, Annotated, Sequence
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI

# é…ç½® DeepSeek API
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not DEEPSEEK_API_KEY:
    raise ValueError("DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# åˆå§‹åŒ– DeepSeek æ¨¡å‹
llm = ChatOpenAI(
    model="deepseek-chat",
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_BASE_URL,
    temperature=0.7,
    max_tokens=1000,
)

print("âœ… å·²åˆå§‹åŒ– DeepSeek æ¨¡å‹: deepseek-chat")

# å®šä¹‰çŠ¶æ€ - ä½¿ç”¨æ ‡å‡†çš„LangGraphæ¶ˆæ¯çŠ¶æ€
class AgentState(TypedDict):
    """AgentçŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯åˆ—è¡¨"""
    messages: Annotated[Sequence[BaseMessage], add_messages]

# å®šä¹‰èŠå¤©èŠ‚ç‚¹
def chat_node(state: AgentState) -> Dict[str, Any]:
    """å¤„ç†èŠå¤©æ¶ˆæ¯çš„èŠ‚ç‚¹"""
    messages = state["messages"]
    
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_message = messages[-1] if messages else None
    if not last_message or not isinstance(last_message, HumanMessage):
        return {"messages": [AIMessage(content="è¯·å‘é€ä¸€æ¡æ¶ˆæ¯ã€‚")]}
    
    print(f"ğŸ’¬ å¤„ç†ç”¨æˆ·æ¶ˆæ¯: {last_message.content}")
    
    # ä½¿ç”¨æµå¼è°ƒç”¨LLM
    print("ğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆå›å¤...")
    ai_response = ""
    
    try:
        # ä½¿ç”¨æµå¼è°ƒç”¨
        for chunk in llm.stream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                content = str(chunk.content) if chunk.content else ""
                ai_response += content
                print(f"ğŸ“ æ”¶åˆ°chunk: {content}")
        
        print(f"âœ… AIæµå¼å›å¤å®Œæˆ: {ai_response[:100]}...")
        
        # è¿”å›AIæ¶ˆæ¯
        return {"messages": [AIMessage(content=ai_response)]}
        
    except Exception as e:
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
        return {"messages": [AIMessage(content=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}")]}

# åˆ›å»ºLangGraphå›¾
def create_graph():
    """åˆ›å»ºæ ‡å‡†çš„LangGraphå›¾"""
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠå¤©èŠ‚ç‚¹
    workflow.add_node("chat", chat_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("chat")
    
    # æ·»åŠ ç»“æŸè¾¹
    workflow.add_edge("chat", END)
    
    # ç¼–è¯‘å›¾ï¼Œä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# åˆ›å»ºå›¾å®ä¾‹
graph = create_graph()

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="LangGraph Standard Server", version="1.0.0")

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class RunRequest(BaseModel):
    input: dict
    stream_mode: List[str] = ["values"]
    assistant_id: str = "agent"
    thread_id: Optional[str] = None
    config: Optional[dict] = None
    metadata: Optional[dict] = None
    multitask_strategy: Optional[str] = None
    stream_subgraphs: bool = False
    stream_resumable: bool = False
    on_disconnect: str = "continue"

@app.get("/")
async def root():
    return {"message": "LangGraph Standard Server is running"}

@app.get("/info")
async def info():
    return {
        "assistant_id": "agent",
        "graph_id": "agent",
        "status": "running",
        "version": "1.0.0"
    }

@app.post("/threads")
async def create_thread():
    """åˆ›å»ºæ–°çº¿ç¨‹"""
    thread_id = str(uuid.uuid4())
    return {
        "thread_id": thread_id,
        "created_at": datetime.now().isoformat(),
        "metadata": {},
        "status": "idle"
    }

@app.post("/threads/search")
async def search_threads():
    """æœç´¢çº¿ç¨‹"""
    return []

@app.post("/threads/{thread_id}/history")
async def get_thread_history(thread_id: str):
    """è·å–çº¿ç¨‹å†å²"""
    # è¿”å›ç©ºå†å²æ•°ç»„ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨å†…å­˜å­˜å‚¨
    # LangGraph SDKæœŸæœ›çš„æ˜¯æ•°ç»„æ ¼å¼ï¼Œä¸æ˜¯å¯¹è±¡æ ¼å¼
    return []

# æ ‡å‡†çš„LangGraphæµå¼ç«¯ç‚¹
@app.post("/runs/stream")
async def stream_run(request: Request):
    """æ ‡å‡†çš„LangGraphæµå¼è¿è¡Œç«¯ç‚¹"""
    return await _handle_stream_request(request)

# å…¼å®¹æ—§ç‰ˆæœ¬çš„ç«¯ç‚¹
@app.post("/threads/{thread_id}/runs/stream")
async def stream_run_with_thread(thread_id: str, request: Request):
    """å…¼å®¹æ—§ç‰ˆæœ¬çš„æµå¼è¿è¡Œç«¯ç‚¹"""
    return await _handle_stream_request(request, thread_id)

async def _handle_stream_request(request: Request, thread_id: Optional[str] = None):
    """å¤„ç†æµå¼è¯·æ±‚çš„æ ¸å¿ƒé€»è¾‘"""
    try:
        body = await request.json()
        print(f"ğŸ” æ”¶åˆ°æµå¼è¯·æ±‚: {json.dumps(body, indent=2, ensure_ascii=False)}")

        # è§£æè¯·æ±‚
        input_data = body.get("input", {})
        stream_mode = body.get("stream_mode", ["values"])
        request_thread_id = body.get("thread_id") or thread_id
        
        # æå–æ¶ˆæ¯
        messages = input_data.get("messages", [])
        if not messages:
            raise HTTPException(status_code=400, detail="No messages provided")
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        langchain_messages = []
        for msg in messages:
            if msg.get("type") == "human":
                # å¤„ç†contentå­—æ®µï¼Œå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°ç»„
                content = msg.get("content", "")
                if isinstance(content, list):
                    # å¦‚æœæ˜¯æ•°ç»„ï¼Œæå–textå†…å®¹
                    text_content = ""
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            text_content += item.get("text", "")
                    content = text_content
                langchain_messages.append(HumanMessage(content=content))
            elif msg.get("type") == "ai":
                content = msg.get("content", "")
                if isinstance(content, list):
                    text_content = ""
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            text_content += item.get("text", "")
                    content = text_content
                langchain_messages.append(AIMessage(content=content))
        
        print(f"ğŸ’¬ å¤„ç† {len(langchain_messages)} æ¡æ¶ˆæ¯")
        
        # é…ç½®
        config = {
            "configurable": {
                "thread_id": request_thread_id or str(uuid.uuid4())
            }
        }
        
        def serialize_message(msg):
            """åºåˆ—åŒ–æ¶ˆæ¯å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
            if isinstance(msg, BaseMessage):
                return {
                    "id": getattr(msg, 'id', str(uuid.uuid4())),
                    "type": "human" if isinstance(msg, HumanMessage) else "ai",
                    "content": [{"type": "text", "text": str(msg.content)}] if isinstance(msg.content, str) else msg.content
                }
            return msg

        def serialize_chunk(chunk):
            """é€’å½’åºåˆ—åŒ–chunkä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
            if isinstance(chunk, dict):
                serialized = {}
                for key, value in chunk.items():
                    if key == "messages" and isinstance(value, list):
                        serialized[key] = [serialize_message(msg) for msg in value]
                    elif isinstance(value, dict):
                        serialized[key] = serialize_chunk(value)
                    elif isinstance(value, list):
                        serialized[key] = [serialize_chunk(item) if isinstance(item, dict) else serialize_message(item) if isinstance(item, BaseMessage) else item for item in value]
                    elif isinstance(value, BaseMessage):
                        serialized[key] = serialize_message(value)
                    else:
                        serialized[key] = value
                return serialized
            elif isinstance(chunk, BaseMessage):
                return serialize_message(chunk)
            return chunk

        async def generate_stream():
            """ç”Ÿæˆç¬¦åˆLangGraphå®˜æ–¹æ ‡å‡†çš„æµå¼å“åº”"""
            run_id = str(uuid.uuid4())

            try:
                # 1. é¦–å…ˆå‘é€è¿è¡Œå¼€å§‹äº‹ä»¶ï¼ˆç¬¦åˆLangGraphå®˜æ–¹æ ¼å¼ï¼‰
                run_start_event = {
                    "run_id": run_id,
                    "attempt": 1
                }
                yield f"data: {json.dumps(run_start_event, ensure_ascii=False)}\n\n"
                print(f"ğŸ“¡ å‘é€è¿è¡Œå¼€å§‹äº‹ä»¶: {run_start_event}")

                # 2. ä½¿ç”¨LangGraphçš„æ ‡å‡†æµå¼æ–¹æ³•
                # æ³¨æ„ï¼šæ ¹æ®LangGraphæºç ï¼Œå½“stream_modeæ˜¯åˆ—è¡¨æ—¶è¿”å›(mode, payload)ï¼Œå¦åˆ™ç›´æ¥è¿”å›payload
                async for chunk in graph.astream(
                    {"messages": langchain_messages},
                    config=config,
                    stream_mode=stream_mode
                ):
                    print(f"ğŸ” æ”¶åˆ°LangGraph chunk: {type(chunk)} - {chunk}")

                    # æ ¹æ®LangGraphæºç å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                    data_to_process = None

                    # å¦‚æœstream_modeæ˜¯åˆ—è¡¨ï¼ŒLangGraphè¿”å›(mode, payload)å…ƒç»„
                    if isinstance(chunk, tuple) and len(chunk) == 2:
                        stream_mode_name, data = chunk
                        print(f"ğŸ“¡ Stream mode: {stream_mode_name}, Data: {type(data)}")
                        data_to_process = data
                    # å¦‚æœstream_modeä¸æ˜¯åˆ—è¡¨ï¼ŒLangGraphç›´æ¥è¿”å›payload
                    elif isinstance(chunk, dict):
                        print(f"ğŸ“¡ ç›´æ¥æ•°æ®æ ¼å¼: {type(chunk)}")
                        data_to_process = chunk
                    else:
                        print(f"âš ï¸ æœªçŸ¥chunkæ ¼å¼: {type(chunk)}")
                        continue

                    # 3. å¤„ç†æ•°æ®å¹¶å‘é€çŠ¶æ€æ›´æ–°
                    if isinstance(data_to_process, dict):
                        # å¤„ç†messagesæ•°æ®
                        if "messages" in data_to_process:
                            # åºåˆ—åŒ–æ¶ˆæ¯ä¸ºLangGraph SDKæœŸæœ›çš„æ ¼å¼
                            messages_data = []
                            for msg in data_to_process["messages"]:
                                if isinstance(msg, BaseMessage):
                                    msg_data = {
                                        "id": getattr(msg, 'id', str(uuid.uuid4())),
                                        "type": "human" if isinstance(msg, HumanMessage) else "ai",
                                        "content": [{"type": "text", "text": str(msg.content)}]
                                    }
                                    messages_data.append(msg_data)

                            # å‘é€çŠ¶æ€æ›´æ–°ï¼ˆç¬¦åˆLangGraphå®˜æ–¹æ ¼å¼ï¼‰
                            state_update = {"messages": messages_data}
                            yield f"data: {json.dumps(state_update, ensure_ascii=False)}\n\n"
                            print(f"âœ… å‘é€çŠ¶æ€æ›´æ–°: {len(messages_data)} æ¡æ¶ˆæ¯")

                        # å¤„ç†å…¶ä»–ç±»å‹çš„æ•°æ®ï¼ˆå¦‚updatesæ¨¡å¼ï¼‰
                        else:
                            # ç›´æ¥å‘é€æ•°æ®
                            yield f"data: {json.dumps(data_to_process, ensure_ascii=False)}\n\n"
                            print(f"âœ… å‘é€å…¶ä»–æ•°æ®æ›´æ–°: {type(data_to_process)}")

                    # æ·»åŠ å°å»¶è¿Ÿä»¥ç¡®ä¿æµå¼æ•ˆæœ
                    await asyncio.sleep(0.01)

            except Exception as e:
                print(f"âŒ æµå¼å¤„ç†é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                error_chunk = {"error": str(e), "run_id": run_id}
                yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )
        
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ å¯åŠ¨æ ‡å‡†LangGraphæœåŠ¡å™¨...")
    print("ğŸ“ æœåŠ¡å™¨åœ°å€: http://localhost:2024")
    print("ğŸ”— Agent Chat UI å¯ä»¥è¿æ¥åˆ°æ­¤æœåŠ¡å™¨")

    uvicorn.run(
        "langgraph_server_new:app",
        host="0.0.0.0",
        port=2024,
        reload=True
    )
